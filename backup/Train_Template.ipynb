{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /workspace/sunggu/0.Challenge/RSNA2023/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]     =  'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]  =  '2'\n",
    "print(\"CPU 갯수 = \", os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117\n",
    "# !pip freeze > /workspace/sunggu/0.Challenge/requirements.txt\n",
    "!pip install -r requirements.txt\n",
    "!pip install -U nibabel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Fix Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 시드(seed) 설정\n",
    "seed = 42\n",
    "\n",
    "# Python의 random 모듈 시드 설정\n",
    "random.seed(seed)\n",
    "\n",
    "# Numpy 시드 설정\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Torch 시드 설정\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import skimage\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from pydicom.pixel_data_handlers.util import apply_modality_lut, apply_voi_lut\n",
    "\n",
    "\n",
    "def list_sort_nicely(l):\n",
    "    def convert(text): return int(text) if text.isdigit() else text\n",
    "    def alphanum_key(key): return [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "    return sorted(l, key=alphanum_key)\n",
    "\n",
    "class RSNA_Dataset(Dataset):\n",
    "    def __init__(self, data_dir, csv_file, target_class, mode=\"train\"):\n",
    "        self.data_dir = data_dir\n",
    "        self.csv_file = csv_file\n",
    "        self.target_class = target_class\n",
    "        self.mode = mode\n",
    "\n",
    "        # CSV 파일에서 데이터 로드\n",
    "        self.df = pd.read_csv(os.path.join(self.data_dir, self.csv_file))\n",
    "        \n",
    "        # 해당하는 모드에 따라 데이터 필터링\n",
    "        self.filtered_df = self.df[self.df['mode'] == self.mode].reset_index(drop=True)\n",
    "\n",
    "        # Albumentations 변환 함수 정의\n",
    "        self.transforms = self.get_transforms()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filtered_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 이미지 경로 및 라벨 가져오기\n",
    "        img_path = self.filtered_df['path'].iloc[idx]\n",
    "        label = self.filtered_df[self.target_class].iloc[idx]\n",
    "\n",
    "        # DICOM 파일 읽기 및 처리\n",
    "        dcm_data = pydicom.dcmread(img_path)\n",
    "        temp_img = apply_modality_lut(dcm_data.pixel_array, dcm_data)\n",
    "        image = apply_voi_lut(temp_img, dcm_data)\n",
    "\n",
    "        # 라벨을 Tensor로 변환\n",
    "        label = torch.tensor(label).float().unsqueeze(0)\n",
    "\n",
    "        # 채널 차원 추가\n",
    "        image = np.expand_dims(image, axis=-1)\n",
    "\n",
    "        # Albumentations 변환 수행\n",
    "        transformed = self.transforms(image=image)\n",
    "        image = transformed['image']\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def get_transforms(self):\n",
    "        if self.mode == \"train\":\n",
    "            return A.Compose([\n",
    "                A.Resize(224, 224),\n",
    "                A.Lambda(image=self.min_max_normalization, always_apply=True),\n",
    "                A.Lambda(image=self.change_to_uint8, always_apply=True),\n",
    "                A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), always_apply=True),\n",
    "                A.Lambda(image=self.change_to_float32, always_apply=True),\n",
    "                \n",
    "                # Augmentation\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.RandomBrightnessContrast(p=0.5),\n",
    "                A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=30, p=0.5),\n",
    "                A.InvertImg(p=0.5),\n",
    "\n",
    "                # Normalize\n",
    "                A.Lambda(image=self.min_max_normalization, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=1.0, mean=0.5, std=0.5),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "        else:\n",
    "            return A.Compose([\n",
    "                A.Resize(224, 224),\n",
    "                A.Lambda(image=self.min_max_normalization, always_apply=True),\n",
    "                A.Lambda(image=self.change_to_uint8, always_apply=True),\n",
    "                A.Lambda(image=self.fixed_clahe, always_apply=True),\n",
    "                A.Lambda(image=self.change_to_float32, always_apply=True),\n",
    "\n",
    "                # Normalize\n",
    "                A.Lambda(image=self.min_max_normalization, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=1.0, mean=0.5, std=0.5),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "\n",
    "    def min_max_normalization(self, image, **kwargs):\n",
    "        if np.unique(image).size == 1:\n",
    "            return image    \n",
    "        image = image.astype('float32')\n",
    "        image -= image.min()\n",
    "        image /= image.max()\n",
    "        return image\n",
    "\n",
    "    def fixed_clahe(self, image, **kwargs):\n",
    "        clahe_mat = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        return clahe_mat.apply(image)\n",
    "\n",
    "    def change_to_uint8(self, image, **kwargs):\n",
    "        return skimage.util.img_as_ubyte(image)\n",
    "\n",
    "    def change_to_float32(self, image, **kwargs):\n",
    "        return skimage.util.img_as_float32(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/workspace/sunggu/0.Challenge/Med_tutorial_ChatGPT/dataset'\n",
    "csv_file = 'rsna_data.csv'\n",
    "target_class = 'cancer'\n",
    "\n",
    "# 학습 데이터셋 생성\n",
    "train_dataset = RSNA_Dataset(data_dir, csv_file, target_class, mode='train')\n",
    "\n",
    "# 검증 데이터셋 생성\n",
    "valid_dataset = RSNA_Dataset(data_dir, csv_file, target_class, mode='valid')\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=200, shuffle=True, num_workers=40)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.model = torchvision.models.resnet50(pretrained=pretrained)\n",
    "        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.model.fc = nn.Linear(2048, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunggu/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sunggu/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of learnable parameters:  23503809\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model = ResNet50()\n",
    "num_params = count_parameters(model)\n",
    "print(\"Number of learnable parameters: \", num_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and scheduler\n",
    "\n",
    "from optimizers import get_optimizer\n",
    "optimizer = get_optimizer(name='adamw', model=model, lr=1e-4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. LR scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define learning rate scheduler\n",
    "\n",
    "# Use a learning rate scheduler to reduce the learning rate over time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Check the resume point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_checkpoint\n",
    "\n",
    "start_epoch     = 0\n",
    "total_epoch     = 1000\n",
    "checkpoint_path = '/workspace/sunggu/0.Challenge/RSNA2023/checkpoints/230530_ResNet50'\n",
    "save_dir        = '/workspace/sunggu/0.Challenge/RSNA2023/predictions/230530_ResNet50'\n",
    "\n",
    "# make folder if not exist\n",
    "os.makedirs(checkpoint_path, exist_ok =True)\n",
    "os.makedirs(save_dir, exist_ok =True)\n",
    "\n",
    "# Resume\n",
    "print(\"Loading... Resume\")\n",
    "start_epoch, best_loss, model, optimizer, scheduler = load_checkpoint(model, optimizer, scheduler, filename='checkpoint.pth')\n",
    "\n",
    "# Optimizer Error fix...!\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if torch.is_tensor(v):\n",
    "            state[k] = v.cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Using the DataParallel for multi-gpu training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the DataParallel for multi-gpu training\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = torch.nn.DataParallel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
    "\n",
    "# sensitivity_score, specificity_score\n",
    "def sensitivity_score(y_true, y_pred):\n",
    "    y_true_pos = y_true[y_true == 1.0]\n",
    "    y_pred_pos = y_pred[y_true == 1.0]\n",
    "\n",
    "    # 길이 확인\n",
    "    if len(y_true) != len(y_pred):\n",
    "        print(\"Error: y_true와 y_pred의 길이가 다릅니다.\")\n",
    "    # 리스트 또는 배열 형태 확인\n",
    "    if not isinstance(y_true, (list, np.ndarray)):\n",
    "        print(\"Error: y_true는 리스트나 배열 형태로 전달되어야 합니다.\")\n",
    "    if not isinstance(y_pred, (list, np.ndarray)):\n",
    "        print(\"Error: y_pred는 리스트나 배열 형태로 전달되어야 합니다.\")\n",
    "\n",
    "    return accuracy_score(y_true_pos, y_pred_pos)\n",
    "\n",
    "def specificity_score(y_true, y_pred):\n",
    "    y_true_neg = y_true[y_true == 0.0]\n",
    "    y_pred_neg = y_pred[y_true == 0.0]\n",
    "    \n",
    "    # 길이 확인\n",
    "    if len(y_true) != len(y_pred):\n",
    "        print(\"Error: y_true와 y_pred의 길이가 다릅니다.\")\n",
    "    # 리스트 또는 배열 형태 확인\n",
    "    if not isinstance(y_true, (list, np.ndarray)):\n",
    "        print(\"Error: y_true는 리스트나 배열 형태로 전달되어야 합니다.\")\n",
    "    if not isinstance(y_pred, (list, np.ndarray)):\n",
    "        print(\"Error: y_pred는 리스트나 배열 형태로 전달되어야 합니다.\")\n",
    "\n",
    "    return accuracy_score(y_true_neg, y_pred_neg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Training & Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "class AverageMeter:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = defaultdict(lambda: {'sum': 0, 'count': 0})\n",
    "\n",
    "    def update(self, key, value, n):\n",
    "        self.data[key]['sum']   += value * n\n",
    "        self.data[key]['count'] += n\n",
    "    \n",
    "    def average(self):\n",
    "        return {k: v['sum'] / v['count'] for k, v in self.data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train the network\n",
    "def train_loop_fn(train_loader, model, criterion, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    metric_logger  = AverageMeter()      # Initialize the network\n",
    "    epoch_iterator = tqdm(train_loader, desc=\"Training (X / X Steps) (loss=X.X)\", dynamic_ncols=True, total=len(train_loader))\n",
    "\n",
    "    for step, batch_data in enumerate(epoch_iterator):\n",
    "\n",
    "        image, target = batch_data\n",
    "        image, target = image.to(device), target.to(device)\n",
    "\n",
    "        logit = model(image)     # Forward pass\n",
    "        loss  = criterion(logit, target)  # Compute the loss\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "\n",
    "        optimizer.zero_grad()  # Clear the gradients\n",
    "        loss.backward()   # Backward pass\n",
    "        optimizer.step()    # Update the weights\n",
    "\n",
    "        # Log the training and validation performance\n",
    "        metric_logger.update(key='train_loss', value=loss_value, n=image.shape[0])\n",
    "        metric_logger.update(key='lr', value=optimizer.param_groups[0][\"lr\"], n=1)    \n",
    "        epoch_iterator.set_description(\"Training: Epochs %d (%d / %d Steps), (train_loss=%2.5f)\" % (epoch, step, len(train_loader), loss_value))\n",
    "\n",
    "    return {k: round(v, 7) for k, v in metric_logger.average().items()}\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_loop_fn(valid_loader, model, criterion, device, epoch, save_dir):\n",
    "    model.eval()\n",
    "    metric_logger  = AverageMeter()\n",
    "    epoch_iterator = tqdm(valid_loader, desc=\"Validating (X / X Steps) (loss=X.X)\", dynamic_ncols=True, total=len(valid_loader))\n",
    "\n",
    "    preds = [] # must be 1d list or array\n",
    "    gts   = [] # must be 1d list or array\n",
    "    for step, batch_data in enumerate(epoch_iterator):\n",
    "        image, target = batch_data\n",
    "        image, target = image.to(device), target.to(device)\n",
    "        logit = model(image)\n",
    "        loss  = criterion(logit, target)\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "\n",
    "        metric_logger.update(key='valid_loss', value=loss_value, n=image.shape[0])\n",
    "        epoch_iterator.set_description(\"Validating: Epochs %d (%d / %d Steps), (valid_loss=%2.5f)\" % (epoch, step, len(valid_loader), loss_value))\n",
    "\n",
    "        # post-processing\n",
    "        preds.append(logit.sigmoid().squeeze().detach().cpu().numpy())\n",
    "        gts.append(target.squeeze().detach().cpu().numpy())     \n",
    "\n",
    "    # Metric Calculation\n",
    "    preds = np.array(preds)\n",
    "    gts   = np.array(gts)\n",
    "\n",
    "        # AUC\n",
    "    auc = roc_auc_score(gts, preds)\n",
    "    metric_logger.update(key='AUC', value=auc.item(), n=image.shape[0])\n",
    "    \n",
    "        # F1\n",
    "    f1 = f1_score(gts, np.round(preds))\n",
    "    metric_logger.update(key='F1', value=f1.item(), n=image.shape[0])\n",
    "\n",
    "        # Accuracy\n",
    "    acc = accuracy_score(gts, np.round(preds))\n",
    "    metric_logger.update(key='Acc', value=acc.item(), n=image.shape[0])\n",
    "\n",
    "        # Sensitivity\n",
    "    sen = sensitivity_score(gts, np.round(preds))\n",
    "    metric_logger.update(key='Sen', value=sen.item(), n=image.shape[0])        \n",
    "\n",
    "        # Specificity\n",
    "    spe = specificity_score(gts, np.round(preds))\n",
    "    metric_logger.update(key='Spe', value=spe.item(), n=image.shape[0])                \n",
    "\n",
    "    return {k: round(v, 7) for k, v in metric_logger.average().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Test Result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Log 조사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_log(path):\n",
    "    log_list = []\n",
    "    lines = open(path, 'r').read().splitlines() \n",
    "    for line in lines:\n",
    "        log_list.append(eval(line))\n",
    "    return  log_list\n",
    "\n",
    "# log 파일을 읽어옵니다.\n",
    "log_list = read_log(path = '/workspace/sunggu/0.Challenge/RSNA2023/checkpoints/230530_ResNet50/log.txt')\n",
    "\n",
    "# 결과를 저장할 딕셔너리를 생성합니다.\n",
    "result_dict = {}\n",
    "for key in log_list[0].keys():\n",
    "    result_dict[key] = [log[key] for log in log_list]\n",
    "\n",
    "# 그래프를 생성합니다.\n",
    "fig, axs = plt.subplots(len(result_dict.keys()), 1, figsize=(10, len(result_dict.keys())*5))\n",
    "\n",
    "for idx, key in enumerate(result_dict.keys()):\n",
    "    axs[idx].plot(result_dict[key])\n",
    "    axs[idx].set_title(key)\n",
    "    print(\"###########################################################\")\n",
    "    print(\"Metric  = \", key)\n",
    "    \n",
    "    if \"loss\" in key:\n",
    "        print(\"Argsort = \", np.argsort(result_dict[key])[:5])\n",
    "        print(\"Value   = \", np.array(result_dict[key])[np.argsort(result_dict[key])[:5]])\n",
    "    else:\n",
    "        print(\"Argsort = \", np.argsort(result_dict[key])[::-1][:5])\n",
    "        print(\"Value   = \", np.array(result_dict[key])[np.argsort(result_dict[key])[::-1][:5]])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /workspace/sunggu/0.Challenge/RSNA2023/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]     =  'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]  =  '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import glob\n",
    "import torch\n",
    "\n",
    "from create_datasets.RSNA import RSNA_Dataset_TEST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 0. Check my computer's cpu core num\n",
    "print(\"CPU 갯수 = \", os.cpu_count())\n",
    "\n",
    "# 1. Create Dataset\n",
    "test_dataset  = RSNA_Dataset_TEST()\n",
    "\n",
    "# 2. Create DataLoader\n",
    "test_loader  = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_models import get_model\n",
    "    \n",
    "model = get_model(name='Resnet50').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume checkpoint\n",
    "import torch\n",
    "\n",
    "print(\"Loading... Resume\")\n",
    "checkpoint = torch.load('/workspace/sunggu/0.Challenge/RSNA2023/checkpoints/230530_ResNet50/epoch_25_checkpoint.pth', map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])        \n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import test_loop_fn\n",
    "\n",
    "preds, ids = test_loop_fn(test_loader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeeze_ids = [i[0] for i in ids]\n",
    "squeeze_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission for Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df['prediction_id'] = squeeze_ids\n",
    "df['cancer'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_mean = df.groupby('prediction_id').mean()\n",
    "grouped_mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_df = grouped_mean.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save csv\n",
    "reset_df.to_csv('/workspace/sunggu/0.Challenge/RSNA2023/dataset/rsna-breast-cancer-detection/sample_submission_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
