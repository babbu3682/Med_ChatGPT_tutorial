{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/sunggu/7.Mentor/Med_ChatGPT_tutorial\n"
     ]
    }
   ],
   "source": [
    "cd /workspace/sunggu/7.Mentor/Med_ChatGPT_tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jul  1 22:54:18 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.78.01    Driver Version: 525.78.01    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:1A:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    24W / 250W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  Off  | 00000000:1B:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    24W / 250W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-PCIE...  Off  | 00000000:1D:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    23W / 250W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-PCIE...  Off  | 00000000:1E:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    23W / 250W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-PCIE...  Off  | 00000000:3D:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    24W / 250W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-PCIE...  Off  | 00000000:3E:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    24W / 250W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-PCIE...  Off  | 00000000:40:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    36W / 250W |    932MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-PCIE...  Off  | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    24W / 250W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "CPU 갯수 =  32\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]     =  'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]  =  '7'\n",
    "print(\"CPU 갯수 = \", os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117\n",
    "# !pip freeze > /workspace/sunggu/0.Challenge/requirements.txt\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Fix Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[Fix Seed]\n",
    "I want you to act as a AI developer in pytorch and python code for me. \n",
    "Fix the randomness of numpy, pytorch, cuda, and random function for reproducibility.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 시드(seed) 설정\n",
    "seed = 42\n",
    "\n",
    "# Python의 random 모듈 시드 설정\n",
    "random.seed(seed)\n",
    "\n",
    "# Numpy 시드 설정\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Torch 시드 설정\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[Define Dataset]\n",
    "I want you to act as a AI developer in pytorch and python code for me. \n",
    "Please help with creating the dataset class that process image processing and augmentation based on binary classification deep learning framework.\n",
    "\n",
    "===INFO===\n",
    "data_dir = '/workspace/sunggu/0.Challenge/Med_tutorial_ChatGPT/dataset'\n",
    "csv_file = 'rsna_data.csv'\n",
    "target_class = 'cancer'\n",
    "\n",
    "Code for creating the RSNA breast cancer dataset class called \"RSNA_Dataset\". \n",
    "This code is a custom dataset class that inherits \"Dataset\" from PyTorch. \n",
    "The dataset can be initialized in either \"train\" or \"valid\" mode and reads the file path and label information from the corresponding CSV file.\n",
    "\n",
    "Each entry in the dataset is defined as follows:\n",
    "- Read the DICOM file using the image path.\n",
    "- Apply the Modality Lookup Table (LUT) and Value of Interest (VOI) LUT for the pixel array in the DICOM file.\n",
    "- Returns the read image and the corresponding label.\n",
    "\n",
    "In addition to the dataset class, we define an compose transform function that performs data preprocessing and augmentation through the \"get_transforms\" function using Albumentation Library. \n",
    "\n",
    "Please conduct step by step using the following procedure.\n",
    "\t1. In the training mode, perform image resizing (224x224), minmax normalization, change_to_uint8, Contrast Limited Adaptive Histogram Equation (CLAHE), change_to_float32, horizontal flip, brightness and contrast adjustment, Shift-Scale-Rotate conversion, image inversion, minmax normalization, and tensor conversion. \n",
    "    2. In validation mode, perform image resizing (224x224), minmax normalization, change_to_uint8, fixed CLAHE, change_to_float32, minmax normalization, and tensor conversion. \n",
    "\t3. Finally, create \"train_dataset\" and \"valid_dataset\" and create a DataLoader that loads them into \"train_loader\" and \"valid_loader\". \"train_loader\" and \"valid_loader\" can be used for training and validation.\n",
    "\n",
    "Here is a example template:\n",
    "<\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset\n",
    "\n",
    "    class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        # Load the data from the specified directory\n",
    "        # ...\n",
    "        \n",
    "        # Preprocess the data\n",
    "        # ...\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Return the data and labels for the specified index\n",
    "        # ...\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of data samples\n",
    "        # ...\n",
    "\n",
    "    # 1. Create Dataset\n",
    "        # ...\n",
    "    # 2. Create DataLoader\n",
    "        # ...\n",
    ">\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import skimage\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from pydicom.pixel_data_handlers.util import apply_modality_lut, apply_voi_lut\n",
    "\n",
    "\n",
    "def list_sort_nicely(l):\n",
    "    def convert(text): return int(text) if text.isdigit() else text\n",
    "    def alphanum_key(key): return [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "    return sorted(l, key=alphanum_key)\n",
    "\n",
    "class RSNA_Dataset(Dataset):\n",
    "    def __init__(self, data_dir, csv_file, target_class, mode=\"train\"):\n",
    "        self.data_dir = data_dir\n",
    "        self.csv_file = csv_file\n",
    "        self.target_class = target_class\n",
    "        self.mode = mode\n",
    "\n",
    "        # CSV 파일에서 데이터 로드\n",
    "        self.df = pd.read_csv(os.path.join(self.data_dir, self.csv_file))\n",
    "        \n",
    "        # 해당하는 모드에 따라 데이터 필터링\n",
    "        self.filtered_df = self.df[self.df['mode'] == self.mode].reset_index(drop=True)\n",
    "\n",
    "        # Albumentations 변환 함수 정의\n",
    "        self.transforms = self.get_transforms()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filtered_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 이미지 경로 및 라벨 가져오기\n",
    "        img_path = self.filtered_df['path'].iloc[idx]\n",
    "        label = self.filtered_df[self.target_class].iloc[idx]\n",
    "\n",
    "        # DICOM 파일 읽기 및 처리\n",
    "        dcm_data = pydicom.dcmread(img_path)\n",
    "        temp_img = apply_modality_lut(dcm_data.pixel_array, dcm_data)\n",
    "        image = apply_voi_lut(temp_img, dcm_data)\n",
    "\n",
    "        # 라벨을 Tensor로 변환\n",
    "        label = torch.tensor(label).float().unsqueeze(0)\n",
    "\n",
    "        # 채널 차원 추가\n",
    "        image = np.expand_dims(image, axis=-1)\n",
    "\n",
    "        # Albumentations 변환 수행\n",
    "        transformed = self.transforms(image=image)\n",
    "        image = transformed['image']\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def get_transforms(self):\n",
    "        if self.mode == \"train\":\n",
    "            return A.Compose([\n",
    "                A.Resize(224, 224),\n",
    "                A.Lambda(image=self.min_max_normalization, always_apply=True),\n",
    "                A.Lambda(image=self.change_to_uint8, always_apply=True),\n",
    "                A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), always_apply=True),\n",
    "                \n",
    "                # Augmentation\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.5),\n",
    "\n",
    "                # contrast\n",
    "                A.OneOf([\n",
    "                    A.RandomToneCurve(scale=0.3, p=0.5),\n",
    "                    A.RandomBrightnessContrast(brightness_limit=(-0.1, 0.2), contrast_limit=(-0.4, 0.5), brightness_by_max=True, always_apply=False, p=0.5),\n",
    "                    A.InvertImg(p=0.5),\n",
    "                ], p=0.5),\n",
    "\n",
    "                # geometric\n",
    "                A.OneOf(\n",
    "                    [\n",
    "                        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=[-0.15, 0.15], rotate_limit=[-30, 30], interpolation=cv2.INTER_LINEAR, border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=None, shift_limit_x=[-0.1, 0.1], shift_limit_y=[-0.2, 0.2], rotate_method='largest_box', p=0.2),\n",
    "                        A.ElasticTransform(alpha=1, sigma=20, alpha_affine=10, interpolation=cv2.INTER_LINEAR, border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=None, approximate=False, same_dxdy=False, p=0.2),\n",
    "                        A.GridDistortion(num_steps=5, distort_limit=0.3, interpolation=cv2.INTER_LINEAR, border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=None, normalized=True, p=0.2),\n",
    "                    ], p=0.5),\n",
    "\n",
    "                # Normalize\n",
    "                A.Lambda(image=self.change_to_float32, always_apply=True),\n",
    "                A.Lambda(image=self.min_max_normalization, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=1.0, mean=0.5, std=0.5),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "        else:\n",
    "            return A.Compose([\n",
    "                A.Resize(224, 224),\n",
    "                A.Lambda(image=self.min_max_normalization, always_apply=True),\n",
    "                A.Lambda(image=self.change_to_uint8, always_apply=True),\n",
    "                A.Lambda(image=self.fixed_clahe, always_apply=True),\n",
    "                A.Lambda(image=self.change_to_float32, always_apply=True),\n",
    "\n",
    "                # Normalize\n",
    "                A.Lambda(image=self.min_max_normalization, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=1.0, mean=0.5, std=0.5),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "        \n",
    "    def min_max_normalization(self, image, **kwargs):\n",
    "        if np.unique(image).size == 1:\n",
    "            return image    \n",
    "        image = image.astype('float32')\n",
    "        image -= image.min()\n",
    "        image /= image.max()\n",
    "        return image\n",
    "\n",
    "    def fixed_clahe(self, image, **kwargs):\n",
    "        clahe_mat = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        return clahe_mat.apply(image)\n",
    "\n",
    "    def change_to_uint8(self, image, **kwargs):\n",
    "        return skimage.util.img_as_ubyte(image)\n",
    "\n",
    "    def change_to_float32(self, image, **kwargs):\n",
    "        return skimage.util.img_as_float32(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunggu/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 40 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/workspace/sunggu/7.Mentor/Med_ChatGPT_tutorial/dataset'\n",
    "csv_file = 'rsna_data.csv'\n",
    "target_class = 'cancer'\n",
    "\n",
    "# 학습 데이터셋 생성\n",
    "train_dataset = RSNA_Dataset(data_dir, csv_file, target_class, mode='train')\n",
    "\n",
    "# 검증 데이터셋 생성\n",
    "valid_dataset = RSNA_Dataset(data_dir, csv_file, target_class, mode='valid')\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=200, shuffle=True, num_workers=40)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[Build a Neural Network]\n",
    "I want you to act as a deep learning expert and code for me. \n",
    "I already have a “RSNA_Dataset” of Mammography Breast Cancer. \n",
    "Please build a popular or SOTA neural network with pytorch and torchvision.\n",
    "The input channel is 1 not 3.\n",
    "The output channel is 1, because the network is used for binary classification task.\n",
    "Please help with creating the neural network class that predict cancer in binary classification task, and discuss their key features and use cases based on binary classification deep learning framework.\n",
    "\n",
    "Please conduct step by step using the following procedure.\n",
    "\t1. define the network class \"ResNet50\".\n",
    "  2. caculate the number of learnable params.\n",
    "\n",
    "Here is a example template:\n",
    "<\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "\tclass Resnet50(nn.Module):\n",
    "\t    def __init__(self, pretrained=True):\n",
    "\t        super().__init__()\n",
    "\t        ...\n",
    "\t        \n",
    "\t    def forward(self, x):\n",
    "\t        ...\n",
    ">\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel, ViTConfig\n",
    "import torch.nn as nn\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ViT, self).__init__()\n",
    "        \n",
    "        # ViTConfig로 모델의 구조를 설정합니다.\n",
    "        config = ViTConfig.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "        # 설정한 구조에 따른 모델을 로드합니다.\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224', config=config)\n",
    "\n",
    "        # 분류를 위한 Linear layer를 추가합니다.\n",
    "        # transformers의 ViTModel은 CLS 토큰에 대한 정보를 첫 번째 요소로 반환하므로, 이를 활용합니다.\n",
    "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, pixel_values, pixel_mask=None):\n",
    "        # pixel_values는 이미지 데이터이며, pixel_mask는 선택적으로 사용할 수 있습니다.\n",
    "        outputs = self.vit(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "        # 첫 번째 요소는 [CLS] 토큰에 대한 정보입니다.\n",
    "        logits = self.classifier(outputs.last_hidden_state[:,0])\n",
    "        \n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224 were not used when initializing ViTModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of learnable parameters:  86390017\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model = ViT()\n",
    "num_params = count_parameters(model)\n",
    "print(\"Number of learnable parameters: \", num_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[Define Loss function]\n",
    "I want you to act as a deep learning expert and code for me. \n",
    "I already have a “RSNA_Dataset” of Mammography Breast Cancer. \n",
    "Please design a popular loss def function in binary classification task and define the loss as \"criterion\".\n",
    "The target and predict(logit) are given in this function and they should be same shape.\n",
    "The predict is the state before the non-linear activation layer.\n",
    "Give me a simple example of how to use it.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 인기 있는 손실 함수인 이진 교차 엔트로피 손실(Binary Cross Entropy Loss)을 사용합니다.\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 예시 사용법\n",
    "# target = torch.tensor([0, 1, 0, 1])\n",
    "# logit = torch.tensor([-1.2, 2.5, -0.8, 1.7])\n",
    "# loss = criterion(logit, target)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[Define Optimizer]\n",
    "I want you to act as a deep learning expert and code for me. \n",
    "I already have a “RSNA_Dataset” of Mammography Breast Cancer. \n",
    "Please design some popular or SOTA optimizers in binary classification network.\n",
    "Learning rate is 1e-4.\n",
    "weight_dacay is 0.\n",
    "Other optimizer's params are set default.\n",
    "Please discuss optimizer hyperparams' key features and use cases.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=0)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-4, weight_decay=0)\n",
    "optimizer = optim.AdamW(params=model.parameters(), lr=1e-4, weight_decay=5e-2) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. LR scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[Define learning rate(LR) scheduler]\n",
    "I want you to act as a deep learning expert and code for me. \n",
    "Please design some popular or SOTA LR schedulers in binary classification network.\n",
    "LR scheduler's params are set default.\n",
    "Please discuss LR scheduler hyperparams' key features and use cases.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# scheduler = lr_scheduler.StepLR(optimizer)\n",
    "# scheduler = lr_scheduler.CosineAnnealingLR(optimizer)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Check the resume point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n[Resume training]\\nI want you to act as a deep learning expert and code for me. \\nPlease code for loading the model weight from the checkpoint if it is exist.\\n\\n==INFO===\\nstart_epoch     = 0\\ntotal_epoch     = 1000\\ncheckpoint_dir  = '/workspace/sunggu/0.Challenge/Med_tutorial_ChatGPT/checkpoints/230616_ResNet50'\\nsave_dir        = '/workspace/sunggu/0.Challenge/Med_tutorial_ChatGPT/predictions/230616_ResNet50'\\ncheckpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pth')\\n\\nPlease conduct step by step using the following procedure.\\n    1. make checkpoints and prediction folders if not exist (using exist_ok=True)\\n    2. load model, optimizer, scheduler, start_epoch\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "[Resume training]\n",
    "I want you to act as a deep learning expert and code for me. \n",
    "Please code for loading the model weight from the checkpoint if it is exist.\n",
    "\n",
    "==INFO===\n",
    "start_epoch     = 0\n",
    "total_epoch     = 1000\n",
    "checkpoint_dir  = '/workspace/sunggu/0.Challenge/Med_tutorial_ChatGPT/checkpoints/230616_ResNet50'\n",
    "save_dir        = '/workspace/sunggu/0.Challenge/Med_tutorial_ChatGPT/predictions/230616_ResNet50'\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pth')\n",
    "\n",
    "Please conduct step by step using the following procedure.\n",
    "    1. make checkpoints and prediction folders if not exist (using exist_ok=True)\n",
    "    2. load model, optimizer, scheduler, start_epoch\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch     = 0\n",
    "total_epoch     = 200\n",
    "checkpoint_dir  = '/workspace/sunggu/7.Mentor/Med_ChatGPT_tutorial/checkpoints/230701_ResNet50_Aug_L2_ViT'\n",
    "save_dir        = '/workspace/sunggu/7.Mentor/Med_ChatGPT_tutorial/predictions/230701_ResNet50_Aug_L2_ViT'\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pth')\n",
    "\n",
    "import os\n",
    "\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크포인트 파일이 존재하는 경우 모델 가중치 로드\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "print(\"모델 가중치를 체크포인트에서 성공적으로 불러왔습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer Error fix...!\n",
    "# for state in optimizer.state.values():\n",
    "#     for k, v in state.items():\n",
    "#         if torch.is_tensor(v):\n",
    "#             state[k] = v.cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Using the DataParallel for multi-gpu training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[Multi-Gpu Training]\n",
    "I want you to act as a deep learning expert and code for me. \n",
    "Please code for using multi-gpu training (DataParallel) of the model.\n",
    "First, check the cuda if it is available.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DataParallel\n",
    "\n",
    "# CUDA 사용 가능 여부 확인\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# 모델 정의\n",
    "# 이미 \"model\"이 정의되어 있다고 가정합니다.\n",
    "\n",
    "# 모델을 CUDA 장치로 이동\n",
    "model = model.to(device)\n",
    "\n",
    "# 멀티 GPU 학습을 위해 모델을 DataParallel로 감싸기\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[Define Metrics]\n",
    "I want you to act as a deep-learning expert and code for me. \n",
    "Please def function code for metrics in binary classification for cancer detection.\n",
    "Here is what I want metrics: AUC, accuracy, f1, sensitivity, specificity...\n",
    "Give me a simple example of how to use it.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, recall_score, confusion_matrix, precision_score\n",
    "\n",
    "def calculate_metrics(predictions, targets):\n",
    "    # AUC 계산\n",
    "    auc = roc_auc_score(targets, predictions)\n",
    "\n",
    "    # 정확도 계산\n",
    "    accuracy = accuracy_score(targets, np.round(predictions))\n",
    "\n",
    "    # F1 점수 계산\n",
    "    f1 = f1_score(targets, np.round(predictions))\n",
    "\n",
    "    # 민감도 (재현율) 계산\n",
    "    sensitivity = recall_score(targets, np.round(predictions))\n",
    "\n",
    "    # 특이도 계산\n",
    "    tn, fp, fn, tp = confusion_matrix(targets, np.round(predictions)).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    return auc, accuracy, f1, sensitivity, specificity\n",
    "\n",
    "# # 예시 사용법\n",
    "# predictions = [0.8, 0.3, 0.6, 0.9]\n",
    "# targets = [1, 0, 0, 1]\n",
    "# auc, accuracy, f1, sensitivity, specificity = calculate_metrics(predictions, targets)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Training & Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n[Define Training & Validation Loop function]\\nI want you to act as a deep-learning expert and code for me. \\nPlease create the loop def function code for training and validation. \\nIn this code, you will use the AverageMeter class to calculate the average of the metrics, and the train_loop_fn function and the valid_loop_fn function to perform the training and validation process. \\nIn addition, the train_loop_fn function and the valid_loop_fn function will utilize a loss function (criterion) to calculate the loss, which will be used to calculate the metrics using 'calculate_metrics' function (accuracy, F1, AUC, sensitivity, and specificity).\\n\\nPlease conduct step by step using the following procedure.\\n\\t1. understand the AverageMeter class and how it calculates the average of the metrics.\\n\\t2. understand the structure and role of the train_loop_fn and valid_loop_fn functions.\\n\\t3. see how to utilize the loss function (criterion) to calculate loss.\\n\\t4. understand how to calculate the metrics (accuracy, F1, AUC, sensitivity, specificity) using calculate_metrics function used in the train_loop_fn and valid_loop_fn functions.\\n\\t5. train_loader, model, criterion, optimizer, device, epoch are given in train_loop_fn.\\n\\t6. valid_loader, model, criterion, device, epoch are given in valid_loop_fn.\\n    7. analyze the methods (update, average) of AverageMeter that are called by the train_loop_fn and valid_loop_fn functions.\\n\\t8. Implement a training and validation loop based on the given code, and check the resulting metrics.\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "[Define Training & Validation Loop function]\n",
    "I want you to act as a deep-learning expert and code for me. \n",
    "Please create the loop def function code for training and validation. \n",
    "In this code, you will use the AverageMeter class to calculate the average of the metrics, and the train_loop_fn function and the valid_loop_fn function to perform the training and validation process. \n",
    "In addition, the train_loop_fn function and the valid_loop_fn function will utilize a loss function (criterion) to calculate the loss, which will be used to calculate the metrics using 'calculate_metrics' function (accuracy, F1, AUC, sensitivity, and specificity).\n",
    "\n",
    "Please conduct step by step using the following procedure.\n",
    "\t1. understand the AverageMeter class and how it calculates the average of the metrics.\n",
    "\t2. understand the structure and role of the train_loop_fn and valid_loop_fn functions.\n",
    "\t3. see how to utilize the loss function (criterion) to calculate loss.\n",
    "\t4. understand how to calculate the metrics (accuracy, F1, AUC, sensitivity, specificity) using calculate_metrics function used in the train_loop_fn and valid_loop_fn functions.\n",
    "\t5. train_loader, model, criterion, optimizer, device, epoch are given in train_loop_fn.\n",
    "\t6. valid_loader, model, criterion, device, epoch are given in valid_loop_fn.\n",
    "    7. analyze the methods (update, average) of AverageMeter that are called by the train_loop_fn and valid_loop_fn functions.\n",
    "\t8. Implement a training and validation loop based on the given code, and check the resulting metrics.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = defaultdict(lambda: {'sum': 0, 'count': 0})\n",
    "\n",
    "    def update(self, key, value, n):\n",
    "        self.data[key]['sum'] += value * n\n",
    "        self.data[key]['count'] += n\n",
    "    \n",
    "    def average(self):\n",
    "        return {k: v['sum'] / v['count'] for k, v in self.data.items()}\n",
    "\n",
    "def train_loop_fn(train_loader, model, criterion, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    metric_logger = AverageMeter()\n",
    "    # epoch_iterator = tqdm(train_loader, desc=f\"Training (Epoch {epoch})\")\n",
    "    epoch_iterator = tqdm(train_loader, desc=\"Training (X / X Steps) (loss=X.X)\", dynamic_ncols=True, total=len(train_loader))\n",
    "\n",
    "    for step, batch_data in enumerate(epoch_iterator):\n",
    "        image, target = batch_data\n",
    "        image, target = image.to(device), target.to(device)\n",
    "\n",
    "        logit = model(image)\n",
    "        loss = criterion(logit, target)\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        metric_logger.update(key='train_loss', value=loss_value, n=image.shape[0])\n",
    "        epoch_iterator.set_description(\"Training: Epochs %d (%d / %d Steps), (train_loss=%2.5f)\" % (epoch, step, len(train_loader), loss_value))\n",
    "        # epoch_iterator.set_postfix(loss=loss_value)\n",
    "\n",
    "    # return {k: round(v, 7) for k, v in metric_logger.average().items()}\n",
    "    return metric_logger.average()\n",
    "    \n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_loop_fn(valid_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    metric_logger = AverageMeter()\n",
    "    # epoch_iterator = tqdm(valid_loader, desc=\"Validating\")\n",
    "    epoch_iterator = tqdm(valid_loader, desc=\"Validating (X / X Steps) (loss=X.X)\", dynamic_ncols=True, total=len(valid_loader))\n",
    "\n",
    "    preds = []\n",
    "    gts = []\n",
    "    for batch_data in epoch_iterator:\n",
    "        image, target = batch_data\n",
    "        image, target = image.to(device), target.to(device)\n",
    "\n",
    "        logit = model(image)\n",
    "        loss = criterion(logit, target)\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "\n",
    "        metric_logger.update(key='valid_loss', value=loss_value, n=image.shape[0])\n",
    "        # epoch_iterator.set_postfix(loss=loss_value)\n",
    "        epoch_iterator.set_description(\"Validating: (valid_loss=%2.5f)\" % (loss_value))\n",
    "\n",
    "        preds.append(logit.sigmoid().squeeze().detach().cpu().numpy())\n",
    "        gts.append(target.squeeze().detach().cpu().numpy())\n",
    "\n",
    "    preds = np.stack(preds)\n",
    "    gts = np.stack(gts)\n",
    "\n",
    "    # Calculate metrics\n",
    "    auc, accuracy, f1, sensitivity, specificity = calculate_metrics(preds, gts)\n",
    "\n",
    "    metric_logger.update(key='valid_loss', value=loss.item(), n=image.size(0))\n",
    "    metric_logger.update(key='valid_auc', value=auc, n=image.size(0))\n",
    "    metric_logger.update(key='valid_accuracy', value=accuracy, n=image.size(0))\n",
    "    metric_logger.update(key='valid_f1', value=f1, n=image.size(0))\n",
    "    metric_logger.update(key='valid_sensitivity', value=sensitivity, n=image.size(0))\n",
    "    metric_logger.update(key='valid_specificity', value=specificity, n=image.size(0))\n",
    "\n",
    "    return metric_logger.average()\n",
    "    # return {k: round(v, 7) for k, v in metric_logger.average().items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[Implementation Training & Validation]\n",
    "I want you to act as a deep-learning expert and code for me. \n",
    "You are given a code that trains a model using a training loop and performs validation using a validation loop. \n",
    "The code also includes functionality for saving checkpoints, updating the learning rate scheduler, and logging the training and validation statistics. \n",
    "\n",
    "Please conduct step by step using the following procedure:\n",
    "\tStep 1: Import the necessary libraries and modules for the code, including warnings, time, and datetime.\n",
    "\tStep 2: Set up a warning filter to ignore any warnings that occur during execution.\n",
    "\tStep 3: Print a message indicating the start of the training process.\n",
    "\tStep 4: Start a loop that iterates over the specified number of epochs, ranging from the start_epoch to the total_epoch.\n",
    "\tStep 5: Within each epoch, call the train_loop_fn function to perform the training process. Pass the train_loader, model, criterion, optimizer, device, and current epoch as arguments. Store the returned training statistics in a variable.\n",
    "\tStep 6: Print the averaged training statistics.\n",
    "\tStep 7: Call the valid_loop_fn function to perform the validation process. Pass the valid_loader, model, criterion, device, and current epoch as arguments. Also, provide the save_dir where the checkpoints will be saved. Store the returned validation statistics in a variable.\n",
    "\tStep 8: Print the averaged validation statistics.\n",
    "\tStep 9: Adjust the learning rate scheduler using the valid_loss from the validation statistics.\n",
    "\tStep 10: Save a checkpoint of the current model, including the epoch, model_state_dict, optimizer_state_dict, and scheduler_state_dict. The checkpoint should be saved in a designated checkpoint_path with a filename that includes the epoch number.\n",
    "\tStep 11: Log the training and validation statistics, including the epoch, learning rate, and both train and valid statistics. The statistics should be stored in a log file located in the checkpoint_path.\n",
    "\tStep 12: Repeat steps 4-11 until all epochs have been completed.\n",
    "\tStep 13: Calculate the total training time by subtracting the start_time from the current time. Format the total time as a human-readable string.\n",
    "\tStep 14: Print the total training time.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "print(\"학습 시작\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(start_epoch, total_epoch):\n",
    "    train_stats = train_loop_fn(train_loader, model, criterion, optimizer, device, epoch)\n",
    "    print(\"==> 평균 학습 통계: \" + str(train_stats))\n",
    "    \n",
    "    valid_stats = valid_loop_fn(valid_loader, model, criterion, device)\n",
    "    print(\"==> 평균 검증 통계: \" + str(valid_stats))\n",
    "\n",
    "    # 학습률 스케줄러 업데이트\n",
    "    scheduler.step(valid_stats['valid_loss'])\n",
    "\n",
    "    # 체크포인트 저장\n",
    "    checkpoint_path = f\"{save_dir}/epoch_{epoch}_checkpoint.pth\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.module.state_dict() if hasattr(model, 'module') else model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "    }, checkpoint_path)\n",
    "\n",
    "    # 로그 기록\n",
    "    log_stats = {**{f'train_{k}': v for k, v in train_stats.items()}, \n",
    "                 **{f'valid_{k}': v for k, v in valid_stats.items()}, \n",
    "                 'epoch': epoch,\n",
    "                 'lr': optimizer.param_groups[0]['lr']}\n",
    "      \n",
    "    with open(f\"{save_dir}/log.txt\", \"a\") as f:\n",
    "        f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "# 학습 완료\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('학습 시간: {}'.format(total_time_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
