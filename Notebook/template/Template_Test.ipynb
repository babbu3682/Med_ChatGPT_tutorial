{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/babbu3682/Med_ChatGPT_tutorial/blob/main/Notebook/template/Template_Test.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/sunggu/7.Mentor/Med_ChatGPT_tutorial\n"
     ]
    }
   ],
   "source": [
    "cd /workspace/sunggu/7.Mentor/Med_ChatGPT_tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul  4 10:49:38 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P40           Off  | 00000000:1B:00.0 Off |                    0 |\n",
      "| N/A   34C    P8    10W / 250W |      2MiB / 22919MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P40           Off  | 00000000:1C:00.0 Off |                    0 |\n",
      "| N/A   38C    P8    12W / 250W |      2MiB / 22919MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla P40           Off  | 00000000:1D:00.0 Off |                    0 |\n",
      "| N/A   58C    P0   203W / 250W |  21327MiB / 22919MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla P40           Off  | 00000000:1E:00.0 Off |                    0 |\n",
      "| N/A   59C    P0    51W / 250W |  21327MiB / 22919MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla P40           Off  | 00000000:3D:00.0 Off |                    0 |\n",
      "| N/A   65C    P0   118W / 250W |  21943MiB / 22919MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla P40           Off  | 00000000:3F:00.0 Off |                    0 |\n",
      "| N/A   79C    P0    72W / 250W |  21943MiB / 22919MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla P40           Off  | 00000000:40:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    49W / 250W |  21327MiB / 22919MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla P40           Off  | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    49W / 250W |  16087MiB / 22919MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "CPU 갯수 =  80\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]     =  'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]  =  '0'\n",
    "print(\"CPU 갯수 = \", os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117\n",
    "# !pip freeze > /workspace/sunggu/0.Challenge/requirements.txt\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fix Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 시드(seed) 설정\n",
    "seed = 42\n",
    "\n",
    "# Python의 random 모듈 시드 설정\n",
    "random.seed(seed)\n",
    "\n",
    "# Numpy 시드 설정\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Torch 시드 설정\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Log 조사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[Log analysis]\n",
    "I want you to act as an AI developer in pytorch and python code for me. \n",
    "Please help with coding for implementing the ability to read data from a log file and visualize it in a graph.\n",
    "\n",
    "Please conduct step by step using the following procedure:\n",
    "\tStep 1: Import the necessary libraries and modules: glob, numpy, matplotlib.pyplot.\n",
    "\tStep 2: Define a read_log function to read a log file. This function takes a file path as an argument, reads the log file, and returns it as a list.\n",
    "\tStep 3: Read the log file using the read_log function. Specify the path as '/workspace/sunggu/0.Challenge/Med_tutorial_ChatGPT/checkpoints/230615_ResNet50_L2_Reg/log.txt'. The read log file is stored in the log_list variable.\n",
    "\tStep 4: Create a dictionary, result_dict, to store the results. Initialize result_dict with the keys from the first dictionary in log_list.\n",
    "\tStep 5: Extract the value corresponding to each key in result_dict from log_list and save it as a list.\n",
    "\tStep 6: Generate a graph. To visualize the results, use matplotlib.pyplot's subplots function. The number of graphs is equal to the number of keys in result_dict, and they are laid out in a vertical orientation. Set the size of the graph via figsize.\n",
    "\tStep 7: Generate a graph for each key in result_dict through a for loop. axs[idx] indicates the subplot to plot at the position corresponding to idx.\n",
    "\tStep 8: Plot the data on the generated graphs. result_dict[key] takes the data corresponding to key and plots the graph. axs[idx].set_title(key) sets the title of the graph to key.\n",
    "\tStep 9: Display the graph. After adjusting the spacing of the graph with plt.tight_layout(), call plt.show() to output the graph.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "import torch.nn as nn\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import skimage\n",
    "import pydicom\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from pydicom.pixel_data_handlers.util import apply_modality_lut, apply_voi_lut\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def list_sort_nicely(l):\n",
    "    def convert(text): return int(text) if text.isdigit() else text\n",
    "    def alphanum_key(key): return [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "    return sorted(l, key=alphanum_key)\n",
    "\n",
    "def fixed_clahe(image, **kwargs):\n",
    "    clahe_mat = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    return clahe_mat.apply(image)\n",
    "\n",
    "def change_to_uint8(image, **kwargs):\n",
    "    return skimage.util.img_as_ubyte(image)\n",
    "\n",
    "def change_to_float32(image, **kwargs):\n",
    "    return skimage.util.img_as_float32(image)\n",
    "\n",
    "def min_max_normalization(image, **kwargs):\n",
    "    if len(np.unique(image)) != 1:\n",
    "        image = image.astype('float32')\n",
    "        image -= image.min()\n",
    "        image /= image.max() \n",
    "    return image\n",
    "\n",
    "def get_transforms():\n",
    "    # medical augmentation\n",
    "    return A.Compose([\n",
    "        # Preprocessing\n",
    "        A.Resize(224, 224),\n",
    "        A.Lambda(image=min_max_normalization, always_apply=True),\n",
    "        A.Lambda(image=change_to_uint8, always_apply=True),\n",
    "        A.Lambda(image=fixed_clahe, always_apply=True),\n",
    "        A.Lambda(image=change_to_float32, always_apply=True),\n",
    "        \n",
    "        # Normalize\n",
    "        A.Lambda(image=min_max_normalization, always_apply=True),\n",
    "        A.Normalize(max_pixel_value=1.0, mean=0.5, std=0.5),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "\n",
    "class RSNA_Dataset(Dataset):\n",
    "    def __init__(self, mode=\"test\"):\n",
    "        self.root       = '/workspace/sunggu/0.Challenge/Med_tutorial_ChatGPT/dataset/rsna_data.csv'\n",
    "        temp_df         = pd.read_csv(self.root)\n",
    "        self.data_df    = temp_df[temp_df['mode'] == mode]\n",
    "        self.transforms = get_transforms()\n",
    "        print(f\"len of data: {len(self.data_df)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data_df['path'].iloc[idx]\n",
    "        dcm_data = pydicom.dcmread(img_path)\n",
    "        temp_img = apply_modality_lut(dcm_data.pixel_array, dcm_data)   \n",
    "        image    = apply_voi_lut(temp_img, dcm_data)                             \n",
    "\n",
    "        label    = self.data_df['cancer'].iloc[idx]\n",
    "        label    = torch.tensor(label).float().unsqueeze(0)\n",
    "\n",
    "        # add channel axis\n",
    "        image    = np.expand_dims(image, axis=-1)\n",
    "        image    = self.transforms(image=image)['image']\n",
    "        \n",
    "        return image, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. Create Dataset\n",
    "test_dataset = RSNA_Dataset(mode=\"test\")\n",
    "\n",
    "# 2. Create DataLoader\n",
    "test_loader  = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.model = torchvision.models.resnet50(pretrained=pretrained)\n",
    "        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.model.fc = nn.Linear(2048, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model = ResNet50()\n",
    "num_params = count_parameters(model)\n",
    "print(\"Number of learnable parameters: \", num_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Using GPU testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Check the resume point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '/workspace/sunggu/0.Challenge/Med_tutorial_ChatGPT/checkpoints/230615_ResNet50_L2_Reg'\n",
    "save_dir        = '/workspace/sunggu/0.Challenge/Med_tutorial_ChatGPT/predictions/230615_ResNet50_L2_Reg'\n",
    "\n",
    "# make folder if not exist\n",
    "os.makedirs(checkpoint_path, exist_ok =True)\n",
    "os.makedirs(save_dir, exist_ok =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "# Metric  =  valid_F1\n",
    "# Argsort =  [ 7 10 14 12  6]\n",
    "# Value   =  [0.6940299 0.6783217 0.6756757 0.6748971 0.6641791]\n",
    "\n",
    "filename = 'epoch_7_checkpoint.pth'\n",
    "print(\"=> loading checkpoint '{}'\".format(filename))\n",
    "checkpoint  = torch.load(os.path.join(checkpoint_path, filename))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, recall_score, confusion_matrix, precision_score\n",
    "\n",
    "def calculate_metrics(predictions, targets):\n",
    "    # AUC 계산\n",
    "    auc = roc_auc_score(targets, predictions)\n",
    "\n",
    "    # 정확도 계산\n",
    "    accuracy = accuracy_score(targets, np.round(predictions))\n",
    "\n",
    "    # F1 점수 계산\n",
    "    f1 = f1_score(targets, np.round(predictions))\n",
    "\n",
    "    # 민감도 (재현율) 계산\n",
    "    sensitivity = recall_score(targets, np.round(predictions))\n",
    "\n",
    "    # 특이도 계산\n",
    "    tn, fp, fn, tp = confusion_matrix(targets, np.round(predictions)).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    return auc, accuracy, f1, sensitivity, specificity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[Define Test Loop function]\n",
    "I want you to act as an AI developer in pytorch and python code for me. \n",
    "Please help with writing a code that performs testing on a model using a testing loop. \n",
    "In this code, you will use the AverageMeter class to calculate the average of the metrics, and the test_loop_fn function to perform the testing process.\n",
    "The code includes functionalities such as calculating metrics, generating Grad-CAM visualizations, and saving the visualizations.\n",
    "In addition, the test_loop_fn function will utilize a loss function (criterion) to calculate the loss, which will be used to calculate the metrics using 'calculate_metrics' function (accuracy, F1, AUC, sensitivity, and specificity).\n",
    "The model predicts cancer within an image in a binary classification manner.\n",
    "\n",
    "Please conduct step by step using the following procedure:\n",
    "\tStep 1: Import the necessary libraries and modules for the code, including time, math, json, tqdm, datetime, matplotlib.pyplot, and defaultdict.\n",
    "\tStep 2: Define the AverageMeter class, which will be used to calculate the average of metrics.\n",
    "\tStep 3: Define the forward_hook function, which serves as the forward hook for capturing intermediate activations.\n",
    "\tStep 4: Define the backward_hook function, which serves as the backward hook for capturing gradients.\n",
    "\tStep 5: Define the min_max_normalization function, which performs min-max normalization on the input image.\n",
    "\tStep 6: Define the test_loop_fn function, which performs testing using the provided model, criterion, and data. The function takes the test_loader, model, criterion, device, and save_dir as arguments.\n",
    "\tStep 7: Set the model to evaluation mode.\n",
    "\tStep 8: Initialize the metric_logger to track the metrics during testing.\n",
    "\tStep 9: Iterate over the batches in the test_loader using the tqdm iterator.\n",
    "\tStep 10: Get the input images and targets from the batch.\n",
    "\tStep 11: Register the forward_hook and backward_hook on the specified layer of the model (resnet50's layer4[2].conv3).\n",
    "\tStep 12: Pass the input images through the model to obtain the logits.\n",
    "\tStep 13: Calculate the loss using the criterion.\n",
    "\tStep 14: Update the metric_logger with the loss value.\n",
    "\tStep 15: Perform post-processing steps, such as storing the predicted probabilities and ground truth values.\n",
    "\tStep 16: Calculate the Grad-CAM visualization by computing the gradients and activations.\n",
    "\tStep 17: Resize the Grad-CAM heatmap to the original image size.\n",
    "\tStep 18: Visualize the image and Grad-CAM heatmap using matplotlib.pyplot.\n",
    "\tStep 19: Save the visualization to a file in the specified save_dir.\n",
    "\tStep 20: Remove the forward_hook and backward_hook.\n",
    "\tStep 21: Concatenate the predicted probabilities and ground truth values to calculate metrics.\n",
    "\tStep 22: Calculate the metrics (AUC, accuracy, F1, sensitivity, specificity) using the calculate_metrics function.\n",
    "\tStep 23: Update the metric_logger with the calculated metrics.\n",
    "\tStep 24: Return the average of the metrics and the ground truth values.\n",
    "\n",
    "Here is a valid_loop example, so you can refer to it:\n",
    "<\n",
    "@torch.no_grad()\n",
    "def valid_loop_fn(valid_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    metric_logger = AverageMeter()\n",
    "    # epoch_iterator = tqdm(valid_loader, desc=\"Validating\")\n",
    "    epoch_iterator = tqdm(valid_loader, desc=\"Validating (X / X Steps) (loss=X.X)\", dynamic_ncols=True, total=len(valid_loader))\n",
    "\n",
    "    preds = []\n",
    "    gts = []\n",
    "    for batch_data in epoch_iterator:\n",
    "        image, target = batch_data\n",
    "        image, target = image.to(device), target.to(device)\n",
    "\n",
    "        logit = model(image)\n",
    "        loss = criterion(logit, target)\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "\n",
    "        metric_logger.update(key='valid_loss', value=loss_value, n=image.shape[0])\n",
    "        # epoch_iterator.set_postfix(loss=loss_value)\n",
    "        epoch_iterator.set_description(\"Validating: Epochs %d (%d / %d Steps), (valid_loss=%2.5f)\" % (epoch, step, len(valid_loader), loss_value))\n",
    "\n",
    "        preds.append(logit.sigmoid().squeeze().detach().cpu().numpy())\n",
    "        gts.append(target.squeeze().detach().cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(preds)\n",
    "    gts = np.concatenate(gts)\n",
    "\n",
    "    # Calculate metrics\n",
    "    auc, accuracy, f1, sensitivity, specificity = calculate_metrics(preds, gts)\n",
    "\n",
    "    metric_logger.update(key='valid_loss', value=loss.item(), n=image.size(0))\n",
    "    metric_logger.update(key='valid_auc', value=auc, n=image.size(0))\n",
    "    metric_logger.update(key='valid_accuracy', value=accuracy, n=image.size(0))\n",
    "    metric_logger.update(key='valid_f1', value=f1, n=image.size(0))\n",
    "    metric_logger.update(key='valid_sensitivity', value=sensitivity, n=image.size(0))\n",
    "    metric_logger.update(key='valid_specificity', value=specificity, n=image.size(0))\n",
    "\n",
    "    return metric_logger.average()\n",
    ">\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AverageMeter\n",
    "\n",
    "# forward hook 정의\n",
    "\n",
    "# backward hook 정의\n",
    "\n",
    "# min_max_normalization 정의\n",
    "\n",
    "# test_loop_fn 정의\n",
    "def test_loop_fn(valid_loader, model, criterion, device, save_dir):\n",
    "    # Set model to evaluation mode\n",
    "    \n",
    "    # Initialize metric_logger to track metrics during testing \n",
    "\n",
    "    # Iterate over batches in the valid_loader using tqdm iterator\n",
    "    for step, batch_data in enumerate(epoch_iterator):\n",
    "        \n",
    "        # register forward hook and backward hook\n",
    "        \n",
    "        # forward\n",
    "\n",
    "        # post-processing\n",
    "\n",
    "        # remove hook\n",
    "\n",
    "        # get gradients\n",
    "\n",
    "        # upsampling\n",
    "\n",
    "        # 시각화\n",
    "\n",
    "        # hook 제거\n",
    "\n",
    "    # post-processing\n",
    "\n",
    "    # Calculate metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "print(f\"Start training\")\n",
    "start_time = time.time()\n",
    "    \n",
    "test_stats = test_loop_fn(test_loader, model, criterion, device, save_dir)\n",
    "print('==> Averaged Test stats: ' + str(test_stats))\n",
    "\n",
    "# Finish\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
