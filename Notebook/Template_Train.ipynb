{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /workspace/sunggu/0.Challenge/Med_tutorial_ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]     =  'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]  =  '0'\n",
    "print(\"CPU 갯수 = \", os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117\n",
    "# !pip freeze > /workspace/sunggu/0.Challenge/requirements.txt\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Fix Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[Fix Seed]\n",
    "I want you to act as a AI developer in pytorch and python code for me. \n",
    "Fix the randomness of numpy, pytorch, cuda, and random function for reproducibility.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 시드(seed) 설정\n",
    "seed = 42\n",
    "\n",
    "# Python의 random 모듈 시드 설정\n",
    "random.seed(seed)\n",
    "\n",
    "# Numpy 시드 설정\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Torch 시드 설정\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[Define Dataset]\n",
    "I want you to act as a AI developer in pytorch and python code for me. \n",
    "Please help with creating the dataset class that process image processing and augmentation based on binary classification deep learning framework.\n",
    "\n",
    "===INFO===\n",
    "data_dir = '/workspace/sunggu/0.Challenge/Med_tutorial_ChatGPT/dataset'\n",
    "csv_file = 'rsna_data.csv'\n",
    "target_class = 'cancer'\n",
    "\n",
    "Code for creating the RSNA breast cancer dataset class called \"RSNA_Dataset\". \n",
    "This code is a custom dataset class that inherits \"Dataset\" from PyTorch. \n",
    "The dataset can be initialized in either \"train\" or \"valid\" mode and reads the file path and label information from the corresponding CSV file.\n",
    "\n",
    "Each entry in the dataset is defined as follows:\n",
    "- Read the DICOM file using the image path.\n",
    "- Apply the Modality Lookup Table (LUT) and Value of Interest (VOI) LUT for the pixel array in the DICOM file.\n",
    "- Returns the read image and the corresponding label.\n",
    "\n",
    "In addition to the dataset class, we define an compose transform function that performs data preprocessing and augmentation through the \"get_transforms\" function using Albumentation Library. \n",
    "\n",
    "Please conduct step by step using the following procedure.\n",
    "\t1. In the training mode, perform image resizing (224x224), minmax normalization, change_to_uint8, Contrast Limited Adaptive Histogram Equation (CLAHE), change_to_float32, horizontal flip, brightness and contrast adjustment, Shift-Scale-Rotate conversion, image inversion, minmax normalization, and tensor conversion. \n",
    "    2. In validation mode, perform image resizing (224x224), minmax normalization, change_to_uint8, fixed CLAHE, change_to_float32, minmax normalization, and tensor conversion. \n",
    "\t3. Finally, create \"train_dataset\" and \"valid_dataset\" and create a DataLoader that loads them into \"train_loader\" and \"valid_loader\". \"train_loader\" and \"valid_loader\" can be used for training and validation.\n",
    "\n",
    "Here is a example template:\n",
    "<\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset\n",
    "\n",
    "    class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        # Load the data from the specified directory\n",
    "        # ...\n",
    "        \n",
    "        # Preprocess the data\n",
    "        # ...\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Return the data and labels for the specified index\n",
    "        # ...\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of data samples\n",
    "        # ...\n",
    "\n",
    "    # 1. Create Dataset\n",
    "        # ...\n",
    "    # 2. Create DataLoader\n",
    "        # ...\n",
    ">\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create Dataset\n",
    "class RSNA_Dataset(Dataset):\n",
    "    def __init__(self, data_dir, csv_file, target_class, mode=\"train\"):\n",
    "        # 데이터 디렉토리 및 CSV 파일 경로\n",
    "\n",
    "        # CSV 파일에서 데이터 로드\n",
    "        \n",
    "        # 해당하는 모드에 따라 데이터 필터링\n",
    "\n",
    "        # Albumentations 변환 함수 정의\n",
    "\n",
    "    # 데이터 샘플 수 반환\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 이미지 경로 및 라벨 가져오기\n",
    "\n",
    "        # DICOM 파일 읽기 및 처리\n",
    "\n",
    "        # 라벨을 Tensor로 변환\n",
    "\n",
    "        # 채널 차원 추가\n",
    "\n",
    "        # Albumentations 변환 수행\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def get_transforms(self):\n",
    "        if self.mode == \"train\":\n",
    "            return A.Compose([\n",
    "                # Preprocessing\n",
    "                \n",
    "                # Augmentation\n",
    "\n",
    "                # Normalize\n",
    "            ])\n",
    "        else:\n",
    "            return A.Compose([\n",
    "                # Preprocessing\n",
    "\n",
    "                # Normalize\n",
    "            ])\n",
    "\n",
    "    # Define normalization\n",
    "\n",
    "    # Contrast Limited Adaptive Histogram Equation (CLAHE)\n",
    "\n",
    "    # Change dtype to uint8\n",
    "\n",
    "    # Change dtype to float32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create DataLoader\n",
    "\n",
    "# 데이터셋 경로 및 파일명\n",
    "\n",
    "# 학습 데이터셋 생성\n",
    "\n",
    "# 검증 데이터셋 생성\n",
    "\n",
    "# DataLoader 생성"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[Build a Neural Network]\n",
    "I want you to act as a deep learning expert and code for me. \n",
    "I already have a “RSNA_Dataset” of Mammography Breast Cancer. \n",
    "Please build a popular or SOTA neural network with pytorch and torchvision.\n",
    "The input channel is 1 not 3.\n",
    "The output channel is 1, because the network is used for binary classification task.\n",
    "Please help with creating the neural network class that predict cancer in binary classification task, and discuss their key features and use cases based on binary classification deep learning framework.\n",
    "\n",
    "Please conduct step by step using the following procedure.\n",
    "\t1. define the network class \"ResNet50\".\n",
    "\t2. caculate the number of learnable params.\n",
    "\n",
    "Here is a example template:\n",
    "<\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "\tclass Resnet50(nn.Module):\n",
    "\t    def __init__(self, pretrained=True):\n",
    "\t        super().__init__()\n",
    "\t        ...\n",
    "\t        \n",
    "\t    def forward(self, x):\n",
    "\t        ...\n",
    ">\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "# 1. Define the network class \"ResNet50\"\n",
    "\n",
    "# 2. Caculate the number of learnable params\n",
    "\n",
    "# Create a model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[Define Loss function]\n",
    "I want you to act as a deep learning expert and code for me. \n",
    "I already have a “RSNA_Dataset” of Mammography Breast Cancer. \n",
    "Please design a popular loss def function in binary classification task and define the loss as \"criterion\".\n",
    "The target and predict(logit) are given in this function and they should be same shape.\n",
    "The predict is the state before the non-linear activation layer.\n",
    "Give me a simple example of how to use it.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인기 있는 손실 함수인 이진 교차 엔트로피 손실(Binary Cross Entropy Loss)을 사용합니다.\n",
    "\n",
    "# 예시 사용법\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[Define Optimizer]\n",
    "I want you to act as a deep learning expert and code for me. \n",
    "I already have a “RSNA_Dataset” of Mammography Breast Cancer. \n",
    "Please design some popular or SOTA optimizers in binary classification network.\n",
    "Learning rate is 1e-4.\n",
    "weight_dacay is 0.\n",
    "Other optimizer's params are set default.\n",
    "Please discuss optimizer hyperparams' key features and use cases.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=0)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-4, weight_decay=0)\n",
    "# optimizer = optim.AdamW(params=model.parameters(), lr=1e-4, weight_decay=0)    # 230616 model\n",
    "# optimizer = optim.AdamW(params=model.parameters(), lr=1e-4, weight_decay=5e-2) # 230615 model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. LR scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[Define learning rate(LR) scheduler]\n",
    "I want you to act as a deep learning expert and code for me. \n",
    "Please design some popular or SOTA LR schedulers in binary classification network.\n",
    "LR scheduler's params are set default.\n",
    "Please discuss LR scheduler hyperparams' key features and use cases.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# scheduler = lr_scheduler.StepLR(optimizer)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "# scheduler = lr_scheduler.CosineAnnealingLR(optimizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Check the resume point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[Resume training]\n",
    "I want you to act as a deep learning expert and code for me. \n",
    "Please code for loading the model weight from the checkpoint if it is exist.\n",
    "\n",
    "==INFO===\n",
    "start_epoch     = 0\n",
    "total_epoch     = 1000\n",
    "checkpoint_dir  = '/workspace/sunggu/0.Challenge/Med_tutorial_ChatGPT/checkpoints/230616_ResNet50'\n",
    "save_dir        = '/workspace/sunggu/0.Challenge/Med_tutorial_ChatGPT/predictions/230616_ResNet50'\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pth')\n",
    "\n",
    "Please conduct step by step using the following procedure.\n",
    "    1. make checkpoints and prediction folders if not exist (using exist_ok=True)\n",
    "    2. load model, optimizer, scheduler, start_epoch\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. make checkpoints and prediction folders if not exist (using exist_ok=True)\n",
    "\n",
    "# 2. load model, optimizer, scheduler, start_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer Error fix...!\n",
    "# for state in optimizer.state.values():\n",
    "#     for k, v in state.items():\n",
    "#         if torch.is_tensor(v):\n",
    "#             state[k] = v.cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Using the DataParallel for multi-gpu training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[Multi-Gpu Training]\n",
    "I want you to act as a deep learning expert and code for me. \n",
    "Please code for using multi-gpu training (DataParallel) of the model.\n",
    "First, check the cuda if it is available.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA 사용 가능 여부 확인\n",
    "\n",
    "# 모델을 CUDA 장치로 이동\n",
    "\n",
    "# 멀티 GPU 학습을 위해 모델을 DataParallel로 감싸기\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[Define Metrics]\n",
    "I want you to act as a deep-learning expert and code for me. \n",
    "Please def function code for metrics in binary classification for cancer detection.\n",
    "Here is what I want metrics: AUC, accuracy, f1, sensitivity, specificity...\n",
    "Give me a simple example of how to use it.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC, accuracy, f1, sensitivity, specificity 계산 함수 \"calculate_metrics\"\n",
    "def calculate_metrics(predictions, targets):\n",
    "    # AUC 계산\n",
    "\n",
    "    # 정확도 계산\n",
    "\n",
    "    # F1 점수 계산\n",
    "\n",
    "    # 민감도 (재현율) 계산\n",
    "\n",
    "    # 특이도 계산\n",
    "\n",
    "    return auc, accuracy, f1, sensitivity, specificity\n",
    "\n",
    "# 예시 사용법\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Training & Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[Define Training & Validation Loop function]\n",
    "I want you to act as a deep-learning expert and code for me. \n",
    "Please create the loop def function code for training and validation. \n",
    "In this code, you will use the AverageMeter class to calculate the average of the metrics, and the train_loop_fn function and the valid_loop_fn function to perform the training and validation process. \n",
    "In addition, the train_loop_fn function and the valid_loop_fn function will utilize a loss function (criterion) to calculate the loss, which will be used to calculate the metrics using 'calculate_metrics' function (accuracy, F1, AUC, sensitivity, and specificity).\n",
    "\n",
    "Please conduct step by step using the following procedure.\n",
    "\t1. understand the AverageMeter class and how it calculates the average of the metrics.\n",
    "\t2. understand the structure and role of the train_loop_fn and valid_loop_fn functions.\n",
    "\t3. see how to utilize the loss function (criterion) to calculate loss.\n",
    "\t4. understand how to calculate the metrics (accuracy, F1, AUC, sensitivity, specificity) using calculate_metrics function used in the train_loop_fn and valid_loop_fn functions.\n",
    "\t5. train_loader, model, criterion, optimizer, device, epoch are given in train_loop_fn.\n",
    "\t6. valid_loader, model, criterion, device, epoch are given in valid_loop_fn.\n",
    "    7. analyze the methods (update, average) of AverageMeter that are called by the train_loop_fn and valid_loop_fn functions.\n",
    "\t8. Implement a training and validation loop based on the given code, and check the resulting metrics.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AverageMeter 클래스\n",
    "\n",
    "# train_loop_fn 함수\n",
    "def train_loop_fn(train_loader, model, criterion, optimizer, device, epoch):\n",
    "    # 모델을 train 모드로 설정\n",
    "\n",
    "    # tqdm을 이용한 progress bar 생성 \n",
    "\n",
    "    # train_loader의 batch만큼 반복하여 학습\n",
    "\n",
    "        # batch_data에서 image와 target을 추출\n",
    "\n",
    "        # 모델의 output 계산\n",
    "\n",
    "        # loss가 무한대인 경우 학습 중단\n",
    "\n",
    "        # loss를 이용한 backpropagation\n",
    "\n",
    "        # AverageMeter 클래스의 update 메소드를 이용하여 loss를 업데이트\n",
    "    \n",
    "    # AverageMeter 클래스의 average 메소드를 이용하여 loss의 평균을 계산\n",
    "    \n",
    "\n",
    "# valid_loop_fn 함수\n",
    "@torch.no_grad()\n",
    "def valid_loop_fn(valid_loader, model, criterion, device):\n",
    "    # 모델을 evaluation 모드로 설정\n",
    "\n",
    "    # tqdm을 이용한 progress bar 생성\n",
    "\n",
    "    # valid_loader의 batch만큼 반복하여 검증\n",
    "\n",
    "        # batch_data에서 image와 target을 추출\n",
    "\n",
    "        # 모델의 output 계산\n",
    "\n",
    "        # loss가 무한대인 경우 학습 중단\n",
    "\n",
    "        # AverageMeter 클래스의 update 메소드를 이용하여 loss를 업데이트\n",
    "\n",
    "    # AverageMeter 클래스의 average 메소드를 이용하여 metrics의 평균을 계산\n",
    "\n",
    "    # AverageMeter 클래스의 average 메소드를 이용하여 loss의 평균을 계산\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[Implementation Training & Validation]\n",
    "I want you to act as a deep-learning expert and code for me. \n",
    "You are given a code that trains a model using a training loop and performs validation using a validation loop. \n",
    "The code also includes functionality for saving checkpoints, updating the learning rate scheduler, and logging the training and validation statistics. \n",
    "\n",
    "Please conduct step by step using the following procedure:\n",
    "\tStep 1: Import the necessary libraries and modules for the code, including warnings, time, and datetime.\n",
    "\tStep 2: Set up a warning filter to ignore any warnings that occur during execution.\n",
    "\tStep 3: Print a message indicating the start of the training process.\n",
    "\tStep 4: Start a loop that iterates over the specified number of epochs, ranging from the start_epoch to the total_epoch.\n",
    "\tStep 5: Within each epoch, call the train_loop_fn function to perform the training process. Pass the train_loader, model, criterion, optimizer, device, and current epoch as arguments. Store the returned training statistics in a variable.\n",
    "\tStep 6: Print the averaged training statistics.\n",
    "\tStep 7: Call the valid_loop_fn function to perform the validation process. Pass the valid_loader, model, criterion, device, and current epoch as arguments. Also, provide the save_dir where the checkpoints will be saved. Store the returned validation statistics in a variable.\n",
    "\tStep 8: Print the averaged validation statistics.\n",
    "\tStep 9: Adjust the learning rate scheduler using the valid_loss from the validation statistics.\n",
    "\tStep 10: Save a checkpoint of the current model, including the epoch, model_state_dict, optimizer_state_dict, and scheduler_state_dict. The checkpoint should be saved in a designated checkpoint_path with a filename that includes the epoch number.\n",
    "\tStep 11: Log the training and validation statistics, including the epoch, learning rate, and both train and valid statistics. The statistics should be stored in a log file located in the checkpoint_path.\n",
    "\tStep 12: Repeat steps 4-11 until all epochs have been completed.\n",
    "\tStep 13: Calculate the total training time by subtracting the start_time from the current time. Format the total time as a human-readable string.\n",
    "\tStep 14: Print the total training time.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 시작 메시지 출력\n",
    "\n",
    "# 학습 통계 기록\n",
    "for epoch in range(start_epoch, total_epoch):\n",
    "    # 학습\n",
    "    \n",
    "    # 검증\n",
    "\n",
    "    # 학습률 스케줄러 업데이트\n",
    "\n",
    "    # 체크포인트 저장\n",
    "\n",
    "    # 로그 기록\n",
    "\n",
    "# 학습 완료\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
