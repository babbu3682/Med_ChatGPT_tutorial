{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/babbu3682/Med_ChatGPT_tutorial/blob/main/Notebook/Hands_On_Train_Aug_L2.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/sunggu/7.Mentor/Med_ChatGPT_tutorial\n"
     ]
    }
   ],
   "source": [
    "cd /workspace/sunggu/7.Mentor/Med_ChatGPT_tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul  3 15:56:43 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P40           Off  | 00000000:1B:00.0 Off |                    0 |\n",
      "| N/A   63C    P0   215W / 250W |  17485MiB / 22919MiB |     92%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P40           Off  | 00000000:1C:00.0 Off |                    0 |\n",
      "| N/A   65C    P0   226W / 250W |  17473MiB / 22919MiB |     52%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla P40           Off  | 00000000:1D:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    50W / 250W |   1075MiB / 22919MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla P40           Off  | 00000000:1E:00.0 Off |                    0 |\n",
      "| N/A   48C    P0    50W / 250W |   1075MiB / 22919MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla P40           Off  | 00000000:3D:00.0 Off |                    0 |\n",
      "| N/A   68C    P0   253W / 250W |  21943MiB / 22919MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla P40           Off  | 00000000:3F:00.0 Off |                    0 |\n",
      "| N/A   59C    P0    63W / 250W |   6295MiB / 22919MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla P40           Off  | 00000000:40:00.0 Off |                    0 |\n",
      "| N/A   44C    P0    58W / 250W |      0MiB / 22919MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla P40           Off  | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   48C    P0    50W / 250W |  22895MiB / 22919MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "CPU 갯수 =  80\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]     =  'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]  =  '6'\n",
    "print(\"CPU 갯수 = \", os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117\n",
    "# !pip freeze > /workspace/sunggu/0.Challenge/requirements.txt\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Fix Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[Fix Seed]\\nI want you to act as a AI developer in pytorch and python code for me. \\nFix the randomness of numpy, pytorch, cuda, and random function for reproducibility.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "[Fix Seed]\n",
    "I want you to act as a AI developer in pytorch and python code for me. \n",
    "Fix the randomness of numpy, pytorch, cuda, and random function for reproducibility.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 시드(seed) 설정\n",
    "seed = 42\n",
    "\n",
    "# Python의 random 모듈 시드 설정\n",
    "random.seed(seed)\n",
    "\n",
    "# Numpy 시드 설정\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Torch 시드 설정\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[Define Dataset]\\nI want you to act as a AI developer in pytorch and python code for me. \\nPlease help with creating the dataset class that process image processing and augmentation based on binary classification deep learning framework.\\n\\n===INFO===\\ndata_dir = \\'/content/drive/MyDrive/Med_ChatGPT_tutorial_Dataset\\'\\ncsv_file = \\'/content/Med_ChatGPT_tutorial/dataset/rsna_data.csv\\'\\n\\ndata_dir = \\'/workspace/sunggu/0.Challenge/Med_tutorial_ChatGPT/dataset\\'\\ncsv_file = \\'rsna_data.csv\\'\\ntarget_class = \\'cancer\\'\\n\\nCode for creating the RSNA breast cancer dataset class called \"RSNA_Dataset\". \\nThis code is a custom dataset class that inherits \"Dataset\" from PyTorch. \\nThe dataset can be initialized in either \"train\" or \"valid\" mode and reads the file path and label information from the corresponding CSV file.\\n\\nEach entry in the dataset is defined as follows:\\n- Read the DICOM file using the image path.\\n- Apply the Modality Lookup Table (LUT) and Value of Interest (VOI) LUT for the pixel array in the DICOM file.\\n- Returns the read image and the corresponding label.\\n\\nIn addition to the dataset class, we define an compose transform function that performs data preprocessing and augmentation through the \"get_transforms\" function using Albumentation Library. \\n\\nPlease conduct step by step using the following procedure.\\n\\t1. In the training mode, perform image resizing (224x224), minmax normalization, change_to_uint8, Contrast Limited Adaptive Histogram Equation (CLAHE), change_to_float32, horizontal flip, brightness and contrast adjustment, Shift-Scale-Rotate conversion, image inversion, minmax normalization, and tensor conversion. \\n    2. In validation mode, perform image resizing (224x224), minmax normalization, change_to_uint8, fixed CLAHE, change_to_float32, minmax normalization, and tensor conversion. \\n\\t3. Finally, create \"train_dataset\" and \"valid_dataset\" and create a DataLoader that loads them into \"train_loader\" and \"valid_loader\". \"train_loader\" and \"valid_loader\" can be used for training and validation.\\n\\nHere is a example template:\\n<\\n    import torch\\n    from torch.utils.data import Dataset\\n\\n    class MyDataset(Dataset):\\n    def __init__(self, data_dir):\\n        # Load the data from the specified directory\\n        # ...\\n        \\n        # Preprocess the data\\n        # ...\\n\\n    def __getitem__(self, index):\\n        # Return the data and labels for the specified index\\n        # ...\\n\\n    def __len__(self):\\n        # Return the number of data samples\\n        # ...\\n\\n    # 1. Create Dataset\\n        # ...\\n    # 2. Create DataLoader\\n        # ...\\n>\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "[Define Dataset]\n",
    "I want you to act as a AI developer in pytorch and python code for me. \n",
    "Please help with creating the dataset class that process image processing and augmentation based on binary classification deep learning framework.\n",
    "\n",
    "===INFO===\n",
    "data_dir = '/content/drive/MyDrive/Med_ChatGPT_tutorial_Dataset'\n",
    "csv_file = '/content/Med_ChatGPT_tutorial/dataset/rsna_data.csv'\n",
    "\n",
    "data_dir = '/workspace/sunggu/0.Challenge/Med_tutorial_ChatGPT/dataset'\n",
    "csv_file = 'rsna_data.csv'\n",
    "target_class = 'cancer'\n",
    "\n",
    "Code for creating the RSNA breast cancer dataset class called \"RSNA_Dataset\". \n",
    "This code is a custom dataset class that inherits \"Dataset\" from PyTorch. \n",
    "The dataset can be initialized in either \"train\" or \"valid\" mode and reads the file path and label information from the corresponding CSV file.\n",
    "\n",
    "Each entry in the dataset is defined as follows:\n",
    "- Read the DICOM file using the image path.\n",
    "- Apply the Modality Lookup Table (LUT) and Value of Interest (VOI) LUT for the pixel array in the DICOM file.\n",
    "- Returns the read image and the corresponding label.\n",
    "\n",
    "In addition to the dataset class, we define an compose transform function that performs data preprocessing and augmentation through the \"get_transforms\" function using Albumentation Library. \n",
    "\n",
    "Please conduct step by step using the following procedure.\n",
    "\t1. In the training mode, perform image resizing (224x224), minmax normalization, change_to_uint8, Contrast Limited Adaptive Histogram Equation (CLAHE), change_to_float32, horizontal flip, brightness and contrast adjustment, Shift-Scale-Rotate conversion, image inversion, minmax normalization, and tensor conversion. \n",
    "    2. In validation mode, perform image resizing (224x224), minmax normalization, change_to_uint8, fixed CLAHE, change_to_float32, minmax normalization, and tensor conversion. \n",
    "\t3. Finally, create \"train_dataset\" and \"valid_dataset\" and create a DataLoader that loads them into \"train_loader\" and \"valid_loader\". \"train_loader\" and \"valid_loader\" can be used for training and validation.\n",
    "\n",
    "Here is a example template:\n",
    "<\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset\n",
    "\n",
    "    class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        # Load the data from the specified directory\n",
    "        # ...\n",
    "        \n",
    "        # Preprocess the data\n",
    "        # ...\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Return the data and labels for the specified index\n",
    "        # ...\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of data samples\n",
    "        # ...\n",
    "\n",
    "    # 1. Create Dataset\n",
    "        # ...\n",
    "    # 2. Create DataLoader\n",
    "        # ...\n",
    ">\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import skimage\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from pydicom.pixel_data_handlers.util import apply_modality_lut, apply_voi_lut\n",
    "\n",
    "\n",
    "def list_sort_nicely(l):\n",
    "    def convert(text): return int(text) if text.isdigit() else text\n",
    "    def alphanum_key(key): return [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "    return sorted(l, key=alphanum_key)\n",
    "\n",
    "class RSNA_Dataset(Dataset):\n",
    "    def __init__(self, data_dir, csv_file, target_class, mode=\"train\"):\n",
    "        self.data_dir = data_dir\n",
    "        self.csv_file = csv_file\n",
    "        self.target_class = target_class\n",
    "        self.mode = mode\n",
    "\n",
    "        # CSV 파일에서 데이터 로드\n",
    "        self.df = pd.read_csv(os.path.join(self.data_dir, self.csv_file))\n",
    "        \n",
    "        # 해당하는 모드에 따라 데이터 필터링\n",
    "        self.filtered_df = self.df[self.df['mode'] == self.mode].reset_index(drop=True)\n",
    "\n",
    "        # Albumentations 변환 함수 정의\n",
    "        self.transforms = self.get_transforms()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filtered_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 이미지 경로 및 라벨 가져오기\n",
    "        img_path = self.filtered_df['path'].iloc[idx]\n",
    "        label = self.filtered_df[self.target_class].iloc[idx]\n",
    "\n",
    "        # DICOM 파일 읽기 및 처리\n",
    "        dcm_data = pydicom.dcmread(img_path)\n",
    "        temp_img = apply_modality_lut(dcm_data.pixel_array, dcm_data)\n",
    "        image = apply_voi_lut(temp_img, dcm_data)\n",
    "\n",
    "        # 라벨을 Tensor로 변환\n",
    "        label = torch.tensor(label).float().unsqueeze(0)\n",
    "\n",
    "        # 채널 차원 추가\n",
    "        image = np.expand_dims(image, axis=-1)\n",
    "\n",
    "        # Albumentations 변환 수행\n",
    "        transformed = self.transforms(image=image)\n",
    "        image = transformed['image']\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def get_transforms(self):\n",
    "        if self.mode == \"train\":\n",
    "            return A.Compose([\n",
    "                A.Resize(224, 224),\n",
    "                A.Lambda(image=self.min_max_normalization, always_apply=True),\n",
    "                A.Lambda(image=self.change_to_uint8, always_apply=True),\n",
    "                A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), always_apply=True),\n",
    "                \n",
    "                # Augmentation\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.5),\n",
    "\n",
    "                # contrast\n",
    "                A.OneOf([\n",
    "                    A.RandomToneCurve(scale=0.3, p=0.5),\n",
    "                    A.RandomBrightnessContrast(brightness_limit=(-0.1, 0.2), contrast_limit=(-0.4, 0.5), brightness_by_max=True, always_apply=False, p=0.5),\n",
    "                    A.InvertImg(p=0.5),\n",
    "                ], p=0.5),\n",
    "\n",
    "                # geometric\n",
    "                A.OneOf(\n",
    "                    [\n",
    "                        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=[-0.15, 0.15], rotate_limit=[-30, 30], interpolation=cv2.INTER_LINEAR, border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=None, shift_limit_x=[-0.1, 0.1], shift_limit_y=[-0.2, 0.2], rotate_method='largest_box', p=0.2),\n",
    "                        A.ElasticTransform(alpha=1, sigma=20, alpha_affine=10, interpolation=cv2.INTER_LINEAR, border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=None, approximate=False, same_dxdy=False, p=0.2),\n",
    "                        A.GridDistortion(num_steps=5, distort_limit=0.3, interpolation=cv2.INTER_LINEAR, border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=None, normalized=True, p=0.2),\n",
    "                    ], p=0.5),\n",
    "\n",
    "                # Normalize\n",
    "                A.Lambda(image=self.change_to_float32, always_apply=True),\n",
    "                A.Lambda(image=self.min_max_normalization, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=1.0, mean=0.5, std=0.5),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "        else:\n",
    "            return A.Compose([\n",
    "                A.Resize(224, 224),\n",
    "                A.Lambda(image=self.min_max_normalization, always_apply=True),\n",
    "                A.Lambda(image=self.change_to_uint8, always_apply=True),\n",
    "                A.Lambda(image=self.fixed_clahe, always_apply=True),\n",
    "                A.Lambda(image=self.change_to_float32, always_apply=True),\n",
    "\n",
    "                # Normalize\n",
    "                A.Lambda(image=self.min_max_normalization, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=1.0, mean=0.5, std=0.5),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "        \n",
    "    def min_max_normalization(self, image, **kwargs):\n",
    "        if np.unique(image).size == 1:\n",
    "            return image    \n",
    "        image = image.astype('float32')\n",
    "        image -= image.min()\n",
    "        image /= image.max()\n",
    "        return image\n",
    "\n",
    "    def fixed_clahe(self, image, **kwargs):\n",
    "        clahe_mat = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        return clahe_mat.apply(image)\n",
    "\n",
    "    def change_to_uint8(self, image, **kwargs):\n",
    "        return skimage.util.img_as_ubyte(image)\n",
    "\n",
    "    def change_to_float32(self, image, **kwargs):\n",
    "        return skimage.util.img_as_float32(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = '/content/drive/MyDrive/Med_ChatGPT_tutorial_Dataset'\n",
    "# csv_file = '/content/Med_ChatGPT_tutorial/dataset/rsna_data.csv'\n",
    "\n",
    "data_dir = '/workspace/sunggu/7.Mentor/Med_ChatGPT_tutorial/dataset'\n",
    "csv_file = 'rsna_data.csv'\n",
    "\n",
    "target_class = 'cancer'\n",
    "\n",
    "# 학습 데이터셋 생성\n",
    "train_dataset = RSNA_Dataset(data_dir, csv_file, target_class, mode='train')\n",
    "\n",
    "# 검증 데이터셋 생성\n",
    "valid_dataset = RSNA_Dataset(data_dir, csv_file, target_class, mode='valid')\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=200, shuffle=True, num_workers=10)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[Build a Neural Network]\\nI want you to act as a deep learning expert and code for me. \\nI already have a “RSNA_Dataset” of\\xa0Mammography Breast Cancer. \\nPlease build a popular or SOTA neural network with pytorch and torchvision.\\nThe input channel is 1 not 3.\\nThe output channel is 1, because the network is used for binary classification task.\\nPlease help with creating the neural network class that predict cancer in binary classification task, and discuss their key features and use cases based on binary classification deep learning framework.\\n\\nPlease conduct step by step using the following procedure.\\n\\t1. define the network class \"ResNet50\".\\n    2. caculate the number of learnable params.\\n\\nHere is a example template:\\n<\\nimport torch.nn as nn\\nimport torchvision\\n\\n\\tclass Resnet50(nn.Module):\\n\\t    def __init__(self, pretrained=True):\\n\\t        super().__init__()\\n\\t        ...\\n\\t        \\n\\t    def forward(self, x):\\n\\t        ...\\n>\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "[Build a Neural Network]\n",
    "I want you to act as a deep learning expert and code for me. \n",
    "I already have a “RSNA_Dataset” of Mammography Breast Cancer. \n",
    "Please build a popular or SOTA neural network with pytorch and torchvision.\n",
    "The input channel is 1 not 3.\n",
    "The output channel is 1, because the network is used for binary classification task.\n",
    "Please help with creating the neural network class that predict cancer in binary classification task, and discuss their key features and use cases based on binary classification deep learning framework.\n",
    "\n",
    "Please conduct step by step using the following procedure.\n",
    "\t1. define the network class \"ResNet50\".\n",
    "    2. caculate the number of learnable params.\n",
    "\n",
    "Here is a example template:\n",
    "<\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "\tclass Resnet50(nn.Module):\n",
    "\t    def __init__(self, pretrained=True):\n",
    "\t        super().__init__()\n",
    "\t        ...\n",
    "\t        \n",
    "\t    def forward(self, x):\n",
    "\t        ...\n",
    ">\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.model = torchvision.models.resnet50(pretrained=pretrained)\n",
    "        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.model.fc = nn.Linear(2048, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunggu/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sunggu/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of learnable parameters:  23503809\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model = ResNet50()\n",
    "num_params = count_parameters(model)\n",
    "print(\"Number of learnable parameters: \", num_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Multi-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[Multi-Gpu Training]\\nI want you to act as a deep learning expert and code for me. \\nPlease code for using multi-gpu training (DataParallel) of the model.\\nFirst, check the cuda if it is available.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "[Multi-Gpu Training]\n",
    "I want you to act as a deep learning expert and code for me. \n",
    "Please code for using multi-gpu training (DataParallel) of the model.\n",
    "First, check the cuda if it is available.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DataParallel\n",
    "\n",
    "# CUDA 사용 가능 여부 확인\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Multi-GPU\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model)         \n",
    "    model = model.to(device)   \n",
    "else :\n",
    "    model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[Define Loss function]\\nI want you to act as a deep learning expert and code for me. \\nI already have a “RSNA_Dataset” of\\xa0Mammography Breast Cancer. \\nPlease design a popular loss def function in binary classification task and define the loss as \"criterion\".\\nThe target and predict(logit) are given in this function and they should be same shape.\\nThe predict is the state before the non-linear activation layer.\\nGive me a simple example of how to use it.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "[Define Loss function]\n",
    "I want you to act as a deep learning expert and code for me. \n",
    "I already have a “RSNA_Dataset” of Mammography Breast Cancer. \n",
    "Please design a popular loss def function in binary classification task and define the loss as \"criterion\".\n",
    "The target and predict(logit) are given in this function and they should be same shape.\n",
    "The predict is the state before the non-linear activation layer.\n",
    "Give me a simple example of how to use it.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 인기 있는 손실 함수인 이진 교차 엔트로피 손실(Binary Cross Entropy Loss)을 사용합니다.\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 예시 사용법\n",
    "# target = torch.tensor([0, 1, 0, 1])\n",
    "# logit = torch.tensor([-1.2, 2.5, -0.8, 1.7])\n",
    "# loss = criterion(logit, target)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n[Define Optimizer]\\nI want you to act as a deep learning expert and code for me. \\nI already have a “RSNA_Dataset” of\\xa0Mammography Breast Cancer. \\nPlease design some popular or SOTA optimizers in binary classification network.\\nLearning rate is 1e-4.\\nweight_dacay is 0.\\nOther optimizer's params are set default.\\nPlease discuss optimizer hyperparams' key features and use cases.\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "[Define Optimizer]\n",
    "I want you to act as a deep learning expert and code for me. \n",
    "I already have a “RSNA_Dataset” of Mammography Breast Cancer. \n",
    "Please design some popular or SOTA optimizers in binary classification network.\n",
    "Learning rate is 1e-4.\n",
    "weight_dacay is 0.\n",
    "Other optimizer's params are set default.\n",
    "Please discuss optimizer hyperparams' key features and use cases.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=0)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-4, weight_decay=0)\n",
    "optimizer = optim.AdamW(params=model.parameters(), lr=1e-4, weight_decay=5e-2) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. LR scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n[Define learning rate(LR) scheduler]\\nI want you to act as a deep learning expert and code for me. \\nPlease design some popular or SOTA LR schedulers in binary classification network.\\nLR scheduler's params are set default.\\nPlease discuss LR scheduler hyperparams' key features and use cases.\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "[Define learning rate(LR) scheduler]\n",
    "I want you to act as a deep learning expert and code for me. \n",
    "Please design some popular or SOTA LR schedulers in binary classification network.\n",
    "LR scheduler's params are set default.\n",
    "Please discuss LR scheduler hyperparams' key features and use cases.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# scheduler = lr_scheduler.StepLR(optimizer)\n",
    "# scheduler = lr_scheduler.CosineAnnealingLR(optimizer)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Check the resume point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n[Resume training]\\nI want you to act as a deep learning expert and code for me. \\nPlease code for loading the model weight from the checkpoint if it is exist.\\n\\n==INFO===\\nstart_epoch     = 0\\ntotal_epoch     = 1000\\n\\ncheckpoint_dir  = '/workspace/sunggu/7.Mentor/Med_tutorial_ChatGPT/checkpoints/230616_ResNet50'\\nsave_dir        = '/workspace/sunggu/7.Mentor/Med_tutorial_ChatGPT/predictions/230616_ResNet50'\\n\\ncheckpoint_dir  = '/content/Med_ChatGPT_tutorial/checkpoints/230616_ResNet50'\\nsave_dir        = '/content/Med_ChatGPT_tutorial/predictions/230616_ResNet50'\\n\\ncheckpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pth')\\n\\n\\nPlease conduct step by step using the following procedure.\\n    1. make checkpoints and prediction folders if not exist (using exist_ok=True)\\n    2. load model, optimizer, scheduler, start_epoch\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "[Resume training]\n",
    "I want you to act as a deep learning expert and code for me. \n",
    "Please code for loading the model weight from the checkpoint if it is exist.\n",
    "\n",
    "==INFO===\n",
    "start_epoch     = 0\n",
    "total_epoch     = 1000\n",
    "\n",
    "checkpoint_dir  = '/workspace/sunggu/7.Mentor/Med_tutorial_ChatGPT/checkpoints/230616_ResNet50'\n",
    "save_dir        = '/workspace/sunggu/7.Mentor/Med_tutorial_ChatGPT/predictions/230616_ResNet50'\n",
    "\n",
    "checkpoint_dir  = '/content/Med_ChatGPT_tutorial/checkpoints/230616_ResNet50'\n",
    "save_dir        = '/content/Med_ChatGPT_tutorial/predictions/230616_ResNet50'\n",
    "\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pth')\n",
    "\n",
    "\n",
    "Please conduct step by step using the following procedure.\n",
    "    1. make checkpoints and prediction folders if not exist (using exist_ok=True)\n",
    "    2. load model, optimizer, scheduler, start_epoch\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch     = 0\n",
    "total_epoch     = 200\n",
    "\n",
    "checkpoint_dir  = '/workspace/sunggu/7.Mentor/Med_ChatGPT_tutorial/checkpoints/230701_ResNet50_Aug_L2'\n",
    "save_dir        = '/workspace/sunggu/7.Mentor/Med_ChatGPT_tutorial/predictions/230701_ResNet50_Aug_L2'\n",
    "\n",
    "# checkpoint_dir  = '/content/Med_ChatGPT_tutorial/checkpoints/230701_ResNet50_Aug_L2'\n",
    "# save_dir        = '/content/Med_ChatGPT_tutorial/predictions/230701_ResNet50_Aug_L2'\n",
    "\n",
    "import os\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 가중치를 체크포인트에서 성공적으로 불러왔습니다.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = checkpoint_dir + '/' + 'epoch_27_checkpoint.pth'\n",
    "\n",
    "# 체크포인트 파일이 존재하는 경우 모델 가중치 로드\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "print(\"모델 가중치를 체크포인트에서 성공적으로 불러왔습니다.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[Define Metrics]\\nI want you to act as a deep-learning expert and code for me. \\nPlease def function code for metrics in binary classification for cancer detection.\\nHere is what I want metrics: AUC, accuracy, f1, sensitivity, specificity...\\nGive me a simple example of how to use it.\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "[Define Metrics]\n",
    "I want you to act as a deep-learning expert and code for me. \n",
    "Please def function code for metrics in binary classification for cancer detection.\n",
    "Here is what I want metrics: AUC, accuracy, f1, sensitivity, specificity...\n",
    "Give me a simple example of how to use it.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, recall_score, confusion_matrix, precision_score\n",
    "\n",
    "def calculate_metrics(predictions, targets):\n",
    "    # AUC 계산\n",
    "    auc = roc_auc_score(targets, predictions)\n",
    "\n",
    "    # 정확도 계산\n",
    "    accuracy = accuracy_score(targets, np.round(predictions))\n",
    "\n",
    "    # F1 점수 계산\n",
    "    f1 = f1_score(targets, np.round(predictions))\n",
    "\n",
    "    # 민감도 (재현율) 계산\n",
    "    sensitivity = recall_score(targets, np.round(predictions))\n",
    "\n",
    "    # 특이도 계산\n",
    "    tn, fp, fn, tp = confusion_matrix(targets, np.round(predictions)).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    return auc, accuracy, f1, sensitivity, specificity\n",
    "\n",
    "# # 예시 사용법\n",
    "# predictions = [0.8, 0.3, 0.6, 0.9]\n",
    "# targets = [1, 0, 0, 1]\n",
    "# auc, accuracy, f1, sensitivity, specificity = calculate_metrics(predictions, targets)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Training & Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n[Define Training & Validation Loop function]\\nI want you to act as a deep-learning expert and code for me. \\nPlease create the loop def function code for training and validation. \\nIn this code, you will use the AverageMeter class to calculate the average of the metrics, and the train_loop_fn function and the valid_loop_fn function to perform the training and validation process. \\nIn addition, the train_loop_fn function and the valid_loop_fn function will utilize a loss function (criterion) to calculate the loss, which will be used to calculate the metrics using 'calculate_metrics' function (accuracy, F1, AUC, sensitivity, and specificity).\\n\\nPlease conduct step by step using the following procedure.\\n\\t1. understand the AverageMeter class and how it calculates the average of the metrics.\\n\\t2. understand the structure and role of the train_loop_fn and valid_loop_fn functions.\\n\\t3. see how to utilize the loss function (criterion) to calculate loss.\\n\\t4. understand how to calculate the metrics (accuracy, F1, AUC, sensitivity, specificity) using calculate_metrics function used in the train_loop_fn and valid_loop_fn functions.\\n\\t5. train_loader, model, criterion, optimizer, device, epoch are given in train_loop_fn.\\n\\t6. valid_loader, model, criterion, device, epoch are given in valid_loop_fn.\\n    7. analyze the methods (update, average) of AverageMeter that are called by the train_loop_fn and valid_loop_fn functions.\\n\\t8. Implement a training and validation loop based on the given code, and check the resulting metrics.\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "[Define Training & Validation Loop function]\n",
    "I want you to act as a deep-learning expert and code for me. \n",
    "Please create the loop def function code for training and validation. \n",
    "In this code, you will use the AverageMeter class to calculate the average of the metrics, and the train_loop_fn function and the valid_loop_fn function to perform the training and validation process. \n",
    "In addition, the train_loop_fn function and the valid_loop_fn function will utilize a loss function (criterion) to calculate the loss, which will be used to calculate the metrics using 'calculate_metrics' function (accuracy, F1, AUC, sensitivity, and specificity).\n",
    "\n",
    "Please conduct step by step using the following procedure.\n",
    "\t1. understand the AverageMeter class and how it calculates the average of the metrics.\n",
    "\t2. understand the structure and role of the train_loop_fn and valid_loop_fn functions.\n",
    "\t3. see how to utilize the loss function (criterion) to calculate loss.\n",
    "\t4. understand how to calculate the metrics (accuracy, F1, AUC, sensitivity, specificity) using calculate_metrics function used in the train_loop_fn and valid_loop_fn functions.\n",
    "\t5. train_loader, model, criterion, optimizer, device, epoch are given in train_loop_fn.\n",
    "\t6. valid_loader, model, criterion, device, epoch are given in valid_loop_fn.\n",
    "    7. analyze the methods (update, average) of AverageMeter that are called by the train_loop_fn and valid_loop_fn functions.\n",
    "\t8. Implement a training and validation loop based on the given code, and check the resulting metrics.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = defaultdict(lambda: {'sum': 0, 'count': 0})\n",
    "\n",
    "    def update(self, key, value, n):\n",
    "        self.data[key]['sum'] += value * n\n",
    "        self.data[key]['count'] += n\n",
    "    \n",
    "    def average(self):\n",
    "        return {k: v['sum'] / v['count'] for k, v in self.data.items()}\n",
    "\n",
    "def train_loop_fn(train_loader, model, criterion, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    metric_logger = AverageMeter()\n",
    "    # epoch_iterator = tqdm(train_loader, desc=f\"Training (Epoch {epoch})\")\n",
    "    epoch_iterator = tqdm(train_loader, desc=\"Training (X / X Steps) (loss=X.X)\", dynamic_ncols=True, total=len(train_loader))\n",
    "\n",
    "    for step, batch_data in enumerate(epoch_iterator):\n",
    "        image, target = batch_data\n",
    "        image, target = image.to(device), target.to(device)\n",
    "\n",
    "        logit = model(image)\n",
    "        loss = criterion(logit, target)\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        metric_logger.update(key='train_loss', value=loss_value, n=image.shape[0])\n",
    "        epoch_iterator.set_description(\"Training: Epochs %d (%d / %d Steps), (train_loss=%2.5f)\" % (epoch, step, len(train_loader), loss_value))\n",
    "        # epoch_iterator.set_postfix(loss=loss_value)\n",
    "\n",
    "    # return {k: round(v, 7) for k, v in metric_logger.average().items()}\n",
    "    return metric_logger.average()\n",
    "    \n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_loop_fn(valid_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    metric_logger = AverageMeter()\n",
    "    # epoch_iterator = tqdm(valid_loader, desc=\"Validating\")\n",
    "    epoch_iterator = tqdm(valid_loader, desc=\"Validating (X / X Steps) (loss=X.X)\", dynamic_ncols=True, total=len(valid_loader))\n",
    "\n",
    "    preds = []\n",
    "    gts = []\n",
    "    for batch_data in epoch_iterator:\n",
    "        image, target = batch_data\n",
    "        image, target = image.to(device), target.to(device)\n",
    "\n",
    "        logit = model(image)\n",
    "        loss = criterion(logit, target)\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "\n",
    "        metric_logger.update(key='valid_loss', value=loss_value, n=image.shape[0])\n",
    "        # epoch_iterator.set_postfix(loss=loss_value)\n",
    "        epoch_iterator.set_description(\"Validating: (valid_loss=%2.5f)\" % (loss_value))\n",
    "\n",
    "        preds.append(logit.sigmoid().squeeze().detach().cpu().numpy())\n",
    "        gts.append(target.squeeze().detach().cpu().numpy())\n",
    "\n",
    "    preds = np.stack(preds)\n",
    "    gts = np.stack(gts)\n",
    "\n",
    "    # Calculate metrics\n",
    "    auc, accuracy, f1, sensitivity, specificity = calculate_metrics(preds, gts)\n",
    "\n",
    "    metric_logger.update(key='valid_loss', value=loss.item(), n=image.size(0))\n",
    "    metric_logger.update(key='valid_auc', value=auc, n=image.size(0))\n",
    "    metric_logger.update(key='valid_accuracy', value=accuracy, n=image.size(0))\n",
    "    metric_logger.update(key='valid_f1', value=f1, n=image.size(0))\n",
    "    metric_logger.update(key='valid_sensitivity', value=sensitivity, n=image.size(0))\n",
    "    metric_logger.update(key='valid_specificity', value=specificity, n=image.size(0))\n",
    "\n",
    "    return metric_logger.average()\n",
    "    # return {k: round(v, 7) for k, v in metric_logger.average().items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[Implementation Training & Validation]\\nI want you to act as a deep-learning expert and code for me. \\nYou are given a code that trains a model using a training loop and performs validation using a validation loop. \\nThe code also includes functionality for saving checkpoints, updating the learning rate scheduler, and logging the training and validation statistics. \\n\\nPlease conduct step by step using the following procedure:\\n\\tStep 1: Import the necessary libraries and modules for the code, including warnings, time, and datetime.\\n\\tStep 2: Set up a warning filter to ignore any warnings that occur during execution.\\n\\tStep 3: Print a message indicating the start of the training process.\\n\\tStep 4: Start a loop that iterates over the specified number of epochs, ranging from the start_epoch to the total_epoch.\\n\\tStep 5: Within each epoch, call the train_loop_fn function to perform the training process. Pass the train_loader, model, criterion, optimizer, device, and current epoch as arguments. Store the returned training statistics in a variable.\\n\\tStep 6: Print the averaged training statistics.\\n\\tStep 7: Call the valid_loop_fn function to perform the validation process. Pass the valid_loader, model, criterion, device, and current epoch as arguments. Also, provide the save_dir where the checkpoints will be saved. Store the returned validation statistics in a variable.\\n\\tStep 8: Print the averaged validation statistics.\\n\\tStep 9: Adjust the learning rate scheduler using the valid_loss from the validation statistics.\\n\\tStep 10: Save a checkpoint of the current model, including the epoch, model_state_dict, optimizer_state_dict, and scheduler_state_dict. The checkpoint should be saved in a designated checkpoint_path with a filename that includes the epoch number.\\n\\tStep 11: Log the training and validation statistics, including the epoch, learning rate, and both train and valid statistics. The statistics should be stored in a log file located in the checkpoint_path.\\n\\tStep 12: Repeat steps 4-11 until all epochs have been completed.\\n\\tStep 13: Calculate the total training time by subtracting the start_time from the current time. Format the total time as a human-readable string.\\n\\tStep 14: Print the total training time.\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "[Implementation Training & Validation]\n",
    "I want you to act as a deep-learning expert and code for me. \n",
    "You are given a code that trains a model using a training loop and performs validation using a validation loop. \n",
    "The code also includes functionality for saving checkpoints, updating the learning rate scheduler, and logging the training and validation statistics. \n",
    "\n",
    "Please conduct step by step using the following procedure:\n",
    "\tStep 1: Import the necessary libraries and modules for the code, including warnings, time, and datetime.\n",
    "\tStep 2: Set up a warning filter to ignore any warnings that occur during execution.\n",
    "\tStep 3: Print a message indicating the start of the training process.\n",
    "\tStep 4: Start a loop that iterates over the specified number of epochs, ranging from the start_epoch to the total_epoch.\n",
    "\tStep 5: Within each epoch, call the train_loop_fn function to perform the training process. Pass the train_loader, model, criterion, optimizer, device, and current epoch as arguments. Store the returned training statistics in a variable.\n",
    "\tStep 6: Print the averaged training statistics.\n",
    "\tStep 7: Call the valid_loop_fn function to perform the validation process. Pass the valid_loader, model, criterion, device, and current epoch as arguments. Also, provide the save_dir where the checkpoints will be saved. Store the returned validation statistics in a variable.\n",
    "\tStep 8: Print the averaged validation statistics.\n",
    "\tStep 9: Adjust the learning rate scheduler using the valid_loss from the validation statistics.\n",
    "\tStep 10: Save a checkpoint of the current model, including the epoch, model_state_dict, optimizer_state_dict, and scheduler_state_dict. The checkpoint should be saved in a designated checkpoint_path with a filename that includes the epoch number.\n",
    "\tStep 11: Log the training and validation statistics, including the epoch, learning rate, and both train and valid statistics. The statistics should be stored in a log file located in the checkpoint_path.\n",
    "\tStep 12: Repeat steps 4-11 until all epochs have been completed.\n",
    "\tStep 13: Calculate the total training time by subtracting the start_time from the current time. Format the total time as a human-readable string.\n",
    "\tStep 14: Print the total training time.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: Epochs 28 (9 / 10 Steps), (train_loss=0.51205): 100%|██████████| 10/10 [30:33<00:00, 183.39s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 평균 학습 통계: {'train_loss': 0.4910823348788927}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: (valid_loss=0.52755): 100%|██████████| 232/232 [01:18<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 평균 검증 통계: {'valid_loss': 0.7012771839638692, 'valid_auc': 0.682054713053821, 'valid_accuracy': 0.646551724137931, 'valid_f1': 0.672, 'valid_sensitivity': 0.711864406779661, 'valid_specificity': 0.5789473684210527}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: Epochs 29 (9 / 10 Steps), (train_loss=0.44225): 100%|██████████| 10/10 [10:10<00:00, 61.07s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 평균 학습 통계: {'train_loss': 0.49208817754914386}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: (valid_loss=0.56024): 100%|██████████| 232/232 [01:28<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 평균 검증 통계: {'valid_loss': 0.7026848460571609, 'valid_auc': 0.6827980969372583, 'valid_accuracy': 0.6336206896551724, 'valid_f1': 0.6613545816733067, 'valid_sensitivity': 0.7033898305084746, 'valid_specificity': 0.5614035087719298}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: Epochs 30 (9 / 10 Steps), (train_loss=0.38172): 100%|██████████| 10/10 [11:19<00:00, 67.92s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 평균 학습 통계: {'train_loss': 0.4838358766960531}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: (valid_loss=0.55772): 100%|██████████| 232/232 [01:48<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 평균 검증 통계: {'valid_loss': 0.7038041202525325, 'valid_auc': 0.6804936068986026, 'valid_accuracy': 0.6422413793103449, 'valid_f1': 0.6719367588932806, 'valid_sensitivity': 0.7203389830508474, 'valid_specificity': 0.5614035087719298}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: Epochs 31 (9 / 10 Steps), (train_loss=0.47245): 100%|██████████| 10/10 [13:15<00:00, 79.51s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 평균 학습 통계: {'train_loss': 0.47811902599252326}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: (valid_loss=0.52661): 100%|██████████| 232/232 [11:49<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 평균 검증 통계: {'valid_loss': 0.703125184331421, 'valid_auc': 0.6821290514421647, 'valid_accuracy': 0.6379310344827587, 'valid_f1': 0.664, 'valid_sensitivity': 0.7033898305084746, 'valid_specificity': 0.5701754385964912}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: Epochs 32 (9 / 10 Steps), (train_loss=0.33028): 100%|██████████| 10/10 [14:24<00:00, 86.47s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 평균 학습 통계: {'train_loss': 0.46655868042107534}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: (valid_loss=0.53472): 100%|██████████| 232/232 [01:25<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 평균 검증 통계: {'valid_loss': 0.70643708769276, 'valid_auc': 0.6816086827237585, 'valid_accuracy': 0.6293103448275862, 'valid_f1': 0.6614173228346457, 'valid_sensitivity': 0.711864406779661, 'valid_specificity': 0.543859649122807}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: Epochs 33 (9 / 10 Steps), (train_loss=0.35833): 100%|██████████| 10/10 [14:13<00:00, 85.32s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 평균 학습 통계: {'train_loss': 0.47151530509891054}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: (valid_loss=0.51577): 100%|██████████| 232/232 [01:29<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 평균 검증 통계: {'valid_loss': 0.7022423203564908, 'valid_auc': 0.6818316978887897, 'valid_accuracy': 0.646551724137931, 'valid_f1': 0.672, 'valid_sensitivity': 0.711864406779661, 'valid_specificity': 0.5789473684210527}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (X / X Steps) (loss=X.X):   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "print(\"학습 시작\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(start_epoch, total_epoch):\n",
    "    train_stats = train_loop_fn(train_loader, model, criterion, optimizer, device, epoch)\n",
    "    print(\"==> 평균 학습 통계: \" + str(train_stats))\n",
    "    \n",
    "    valid_stats = valid_loop_fn(valid_loader, model, criterion, device)\n",
    "    print(\"==> 평균 검증 통계: \" + str(valid_stats))\n",
    "\n",
    "    # 학습률 스케줄러 업데이트\n",
    "    scheduler.step(valid_stats['valid_loss'])\n",
    "\n",
    "    # 체크포인트 저장\n",
    "    checkpoint_path = f\"{checkpoint_dir}/epoch_{epoch}_checkpoint.pth\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.module.state_dict() if hasattr(model, 'module') else model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "    }, checkpoint_path)\n",
    "\n",
    "    # 로그 기록\n",
    "    log_stats = {**{f'train_{k}': v for k, v in train_stats.items()}, \n",
    "                 **{f'valid_{k}': v for k, v in valid_stats.items()}, \n",
    "                 'epoch': epoch,\n",
    "                 'lr': optimizer.param_groups[0]['lr']}\n",
    "      \n",
    "    with open(f\"{checkpoint_dir}/log.txt\", \"a\") as f:\n",
    "        f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "# 학습 완료\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('학습 시간: {}'.format(total_time_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
